{
    "agent_id": "9776d496-f6fd-44e0-999b-a3dc95c231a9",
    "workflow_result": {
      "tweet": {
        "tweet_id": "1864149492167290897",
        "created_at": "2024-12-04T08:57:27",
        "full_text": "Research paper by @ScribeHow and @mldcmu\\n\\nhttps://t.co/s0cH0owC94 https://t.co/i6scxS4MeM",
        "language": "unknown",
        "favorite_count": 16,
        "retweet_count": 0,
        "bookmark_count": 18,
        "quote_count": 0,
        "reply_count": 0,
        "views_count": 1726,
        "screen_name": "Saboo_Shubham_",
        "user_name": "Shubham Saboo",
        "profile_image_url": "https://pbs.twimg.com/profile_images/1670107849815478273/Q3oUhtHM_normal.jpg",
        "is_retweet": false,
        "is_quote": false,
        "possibly_sensitive": false,
        "urls": [
          {
            "id": "11515484-1aec-4c87-b790-21b4b4ddab98",
            "index": 0,
            "original_url": "https://arxiv.org/pdf/2411.15004",
            "downloaded_path": "tweet_collection/arxiv/arxiv_27fb86e353_0.pdf",
            "downloaded_path_md": "tweet_collection/arxiv/markdown/arxiv_27fb86e353_0.md",
            "type": "arxiv",
            "domain": "arxiv.org",
            "content_type": ".pdf",
            "file_category": "document",
            "downloaded_at": "2024-12-14T22:00:53"
          }
        ],
        "media": [
          {
            "id": "02f06240-dba5-465a-bdaf-152229cdcf81",
            "type": "photo",
            "original_url": "https://pbs.twimg.com/media/Gd7KFXba0AUNfTV?format=jpg&name=orig",
            "final_url": "https://pbs.twimg.com/media/Gd7KFXba0AUNfTV?format=jpg&name=orig",
            "downloaded_path": "tweet_collection/media/reference_media/photo_385284a1a8.jpg",
            "content_type": "image/jpeg",
            "thumbnail": "https://pbs.twimg.com/media/Gd7KFXba0AUNfTV?format=jpg&name=thumb",
            "downloaded_at": "2024-12-14T22:00:54"
          }
        ]
      },
      "tweet_type": [
        "NonAI",
        "Technical"
      ],
      "tags": [
        "web agents",
        "LLMs",
        "web navigation",
        "AI",
        "machine learning",
        "technical",
        "research",
        "computer science"
      ],
      "blog_style": "Academic",
      "blog_content": "\n# ScribeAgent: A Novel Approach to Building Specialized Web Agents\n\n**Abstract:** This blog post delves into the research paper \"SCRIBEAGENT: TOWARDS SPECIALIZED WEB AGENTS USING PRODUCTION-SCALE WORKFLOW DATA.\"  The authors present a novel approach to building web agents, focusing on fine-tuning open-source Large Language Models (LLMs) with real-world workflow data.  This method demonstrates significant performance improvements over existing prompting-based agents on standard web navigation benchmarks.\n\n**Introduction:**  The increasing complexity of web-based tasks necessitates more sophisticated agents capable of independent navigation and execution.  Existing web agents often rely on general-purpose LLMs like GPT-4, leading to limitations in understanding specialized web contexts like HTML and hindering long-term planning.  The ScribeAgent project offers an alternative by fine-tuning open-source LLMs using a massive dataset of real-world user interactions.\n\n**Key Contributions:**\n\n* **Production-Scale Data:** The core strength of ScribeAgent lies in its use of a substantial, proprietary dataset encompassing over 250 domains and 6 billion tokens.  This real-world data, collected through the Scribe platform, provides a rich and diverse set of web interactions, crucial for effective agent training.\n* **Specialized Fine-tuning:**  Instead of relying on prompting general-purpose LLMs, ScribeAgent fine-tunes open-source LLMs using the collected workflow data.  This approach allows for the development of specialized agents tailored to specific web contexts, leading to better performance compared to generic models.\n* **Direct Action Generation:**  ScribeAgent is designed as a single-stage agent, directly generating the next action based on the current DOM and action history.  This contrasts with previous multi-stage approaches, which often involve intermediate steps for element selection. This streamlined approach improves efficiency.\n* **State-of-the-Art Performance:** The paper demonstrates that ScribeAgent achieves state-of-the-art performance on the Mind2Web and WebArena benchmarks.  Significant improvements are reported over existing text-based web agents, highlighting the effectiveness of the proposed method.\n\n**Methodology:**\n\nThe paper details a four-stage pipeline for task execution:\n\n1. **Objective Refinement:**  The user's high-level objective is refined into a more detailed, step-by-step action plan.\n2. **Action Generation:**  ScribeAgent generates a suggested action based on the current web page and history.\n3. **Action Execution:**  The suggested action is executed by the user.\n4. **Completeness Evaluation:**  The system evaluates if the task is completed.\n\n**Discussion:**\n\nThe paper highlights several important considerations:\n\n* **Context Window Optimization:**  The authors address the limitations of context windows by chunking the DOM and using a refined evaluation procedure.\n* **Data Preprocessing:** The detailed preprocessing steps for the workflow data, including removing non-essential elements, are crucial for model effectiveness.\n* **LLM Selection:**  The paper explores different open-source LLMs, demonstrating that fine-tuning can significantly enhance their performance for web navigation tasks.\n\n**Conclusion:**\n\nThe ScribeAgent project offers a promising avenue for developing robust and efficient web agents.  The use of production-scale, real-world data and specialized fine-tuning techniques leads to substantial performance gains over existing methods.  The insights provided in the paper are valuable for future research in the field of web agent development.  Further research could explore integrating memory components, planning strategies, and multi-modal inputs to enhance the capabilities of web agents.\n\n",
      "status": "BlogGenerated",
      "messages": [
        {
          "content": "You are an expert tweet classifier, you will be given a tweet text, you need to classify the tweet from below selected categories, multi class classification is possible.\n\nCategories:\n['AI','NonAI','Technical','NonTechnical']\n\nDefinitions:\nAI: Content is AI related\nNonAI: Content is not AI related\nTechnical: Content is technical, AI or non AI\nNonTechnical: Content is not technical and can be used for non tech audience (e.g. general AI related discussion on business can be NonTechnical and also AI)\n\nTweet:\nResearch paper by @ScribeHow and @mldcmu\\n\\nhttps://t.co/s0cH0owC94 https://t.co/i6scxS4MeM\n\nRespond inside category tags\n<category>\n[]\n</category>",
          "additional_kwargs": {},
          "response_metadata": {},
          "type": "human",
          "name": null,
          "id": null,
          "example": false
        },
        {
          "content": "<category>\n['NonAI', 'Technical']\n</category>\n",
          "additional_kwargs": {},
          "response_metadata": {
            "prompt_feedback": {
              "block_reason": 0,
              "safety_ratings": []
            },
            "finish_reason": "STOP",
            "safety_ratings": [
              {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "probability": "NEGLIGIBLE",
                "blocked": false
              },
              {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "probability": "NEGLIGIBLE",
                "blocked": false
              },
              {
                "category": "HARM_CATEGORY_HARASSMENT",
                "probability": "NEGLIGIBLE",
                "blocked": false
              },
              {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "probability": "NEGLIGIBLE",
                "blocked": false
              }
            ]
          },
          "type": "ai",
          "name": null,
          "id": "run-0c23c13d-714a-4e27-bc07-6ac51f5205f9-0",
          "example": false,
          "tool_calls": [],
          "invalid_tool_calls": [],
          "usage_metadata": {
            "input_tokens": 176,
            "output_tokens": 16,
            "total_tokens": 192,
            "input_token_details": {
              "cache_read": 0
            }
          }
        },
        {
          "content": "You are an expert copy writer who can write blog posts for Linkedin in Academic style of writing.\n\nYou will be given a short tweet text and reference article, your task is to write a blog post in correct markdown format and add references.\n\nThe type of blog would be of NonAI, Technical categories. Make sure to write the blog inside <blog> tags. Also determine proper hash tags and extract inside <tags> in list form\n\nAdd references section in the end, having reference links mentioed\n\n**Tweet**:\nResearch paper by @ScribeHow and @mldcmu\\n\\nhttps://t.co/s0cH0owC94 https://t.co/i6scxS4MeM\n\n**Article**:\n## SCRIBEAGENT: TOWARDS SPECIALIZED WEB AGENTS USING PRODUCTION-SCALE WORKFLOW DATA\n\nJunhong Shen ∗\n\nAtishay Jain ∗\n\nZedian Xiao ∗\n\nIshan Amlekar\n\nMouad Hadji\n\nAaron Podolny\n\nAmeet Talwalkar\n\n{ junhongs, atalwalkar } @cs.cmu.edu { atishay, mark, ishan, mouad, aaron } @scribehow.com\n\nAI/ML, Scribe Machine Learning Department, CMU\n\n## ABSTRACT\n\nLarge Language Model (LLM) agents are rapidly improving to handle increasingly complex web-based tasks. Most of these agents rely on general-purpose, proprietary models like GPT-4 and focus on designing better prompts to improve their planning abilities. However, general-purpose LLMs are not specifically trained to understand specialized web contexts such as HTML, and they often struggle with long-horizon planning. We explore an alternative approach that fine-tunes open-source LLMs using production-scale workflow data collected from over 250 domains corresponding to 6 billion tokens. This simple yet effective approach shows substantial gains over prompting-based agents on existing benchmarks-ScribeAgent achieves state-of-the-art direct generation performance on Mind2Web and improves the task success rate by 7.3% over the previous best text-only web agents on WebArena. We further perform detailed ablation studies on various fine-tuning design choices and provide insights into LLM selection, training recipes, context window optimization, and effect of dataset sizes.\n\n## 1 INTRODUCTION\n\nLarge language model (LLM) agents have advanced significantly in web navigation. They can carry out user-specified tasks in multiple steps by reasoning on their own what actions to take and what external resources to interface with. Recent studies (Zheng et al., 2024; Lai et al., 2024; Zhang et al., 2024; Song et al., 2024) have shown that, with better planning and exploration strategies, LLM agents can independently solve various web tasks ranging from simple navigation, such as locating a specific Wikipedia page, to more complex operations, such as booking flights or restaurants.\n\nDespite these improvements, the performance of existing web agents on research benchmarks remains significantly below human levels (Deng et al., 2023; Zhou et al., 2024; Drouin et al., 2024). One possible reason is their dependence on general-purpose LLMs. Indeed, all top-performing agents like WebPilot (Zhang et al., 2024), AWM (Wang et al., 2024b), and SteP (Sodhi et al., 2024) rely on prompting proprietary models like GPT-4 (OpenAI, 2024a). These general-purpose LLMs are not optimized for interpreting web contexts such as HTML or accessibility trees; their pretraining and alignment processes do not address navigation-related challenges; and their proprietary nature presents a major obstacle in adapting them to web environments via continual training.\n\nFigure 1: Left: Most existing LLM web agents are built on top of general-purpose, proprietary models like GPT-4 and rely heavily on prompt engineering. Their performance is enhanced by leveraging external planning, reasoning, and memory modules. Right: We explore an alternative way to develop specialized agents by fine-tuning open-source LLMs using a large set of high-quality, realworld workflow data. This significantly boosts agent's navigation and planning capacity, enabling it to outperform proprietary models with a smaller LLM backbone, thereby reducing serving costs.\n\n<!-- image -->\n\nIn this work, we explore an alternative approach by fine-tuning open-source LLMs with a large set of real-world workflow data 1 to develop specialized web agents (Figure 1). Through extensive experiments, we show that this approach not only boosts the web understanding and planning abilities of LLMs, achieving state-of-the-art results on various benchmarks, but also allows us to develop agent models significantly smaller than proprietary LLMs, reducing the serving costs. Our empirical results suggest that large-scale, high-quality, real-world data can be essential to agent development.\n\nSpecifically, we collect a set of proprietary workflow data representing action sequences executed by real users in real web environments through Scribe. This dataset encompasses a large and diverse spectrum of websites (over 250 domains and 10,000 subdomains), task objectives, task difficulty, and task length. Each step in the workflow features not only the raw HTML-DOM of the website but also a comprehensive documentation of the action, including action description in natural language, mouse or keyboard operation, and the CSS selector of the target HTML element. We reformat the data into a next-step prediction formulation and fine-tune a set of open-source LLMs via the parameter-efficient LoRA (Hu et al., 2022). After preprocessing and reformatting, our training dataset contains more than 6 billion tokens.\n\nWith access to this production-scale dataset, we develop ScribeAgent, the first family of specialized, single-stage LLM agents capable of directly generating the next step based on the website's DOM and action history. This is in contrast with previous fine-tuned agents that require multiple stages to produce an action, e.g., first narrowing down to a set of target element candidates and then selecting one from the candidates (Deng et al., 2023). To evaluate the capacity and generalization ability of ScribeAgent, we test it on public benchmarks without any further training. ScribeAgent significantly outperforms existing GPT-4-based and multi-stage agents. Notably, the 32B-parameter ScribeAgent-Large achieves state-of-the-art direct generation performance on Mind2Web (Deng et al., 2023), with step success rate surpassing the baselines by 5-10% across all test sets. On the end-to-end task execution benchmark WebArena (Zhou et al., 2024), our 7B ScribeAgent-Small improves the previous best task success rate from 45.7% to 51.3%. ScribeAgent-Large achieves an even better task success rate of 53%, marking the highest performance among text-only LLM agents.\n\nBeyond the empirical results, our work also provides several insights valuable for future web agent research: (1) we show that direct fine-tuning on highly structured inputs (HTML-DOM) is feasible and can improve the agent's ability in identifying the correct target; (2) we identify an effective HTML preprocessing strategy that balances between preserving essential information and minimizing context length; (3) we provide a thorough analysis on various design choices in fine-tuning, such as LLM backbone and context window selection; (4) we illustrate how fine-tuning improves agent performance as dataset size increases.\n\nOur work highlights the potential of building web agents via specialized fine-tuning with productionscale data. This approach not only improves agents' capabilities relative to prompt-engineered alternatives, but also reduces inference costs due to the smaller sizes of open-source LLMs. While our work focuses on studying the effect of fine-tuning, ScribeAgent can be extended to leverage more sophisticated search or memory modules (Koh et al., 2024; Wang et al., 2024b), combined with existing planning frameworks (Yao et al., 2022; Madaan et al., 2023; Shinn et al., 2023), or integrated into multi-modal web agent systems as the text model (Wang et al., 2024a). We view ScribeAgent as an important step towards developing AI assistants and fully automated agents for real-world web applications.\n\n## 2 RELATED WORK\n\nPrompting-based agent frameworks. The majority of web agent works reuse existing LLMs and propose different prompting strategies to improve action prediction. One line of research focuses on exploiting previous experience via self-feedback (Sun et al., 2023) or in-context demonstrations (Fu et al., 2024; Zheng et al., 2024; Wang et al., 2024b; Ou et al., 2024; Shen et al., 2024b). A separate line of work centers around encouraging exploration by including external evaluators (Pan et al., 2024), using synthesized instructions (Murty et al., 2024b), or applying more sophisticated search algorithms like stack (Sodhi et al., 2024), best-first tree search (Koh et al., 2024), or Monte Carlo Tree Search (Zhang et al., 2024). Despite the research efforts, these prompting methods rely heavily on the quality of the LLM used. Open-source models such as LLaMA (Dubey et al., 2024), Code LLaMA (Rozi'ere et al., 2024), and Flan-T5 (Chung et al., 2022) generally underperform proprietary models like GPT-4. However, fine-tuning proprietary LLMs can often be costly and challenging, as it is restricted to being done through APIs. This implies an opportunity for enhancing open-source LLMs to match or outperform proprietary agents.\n\nFine-tuning-based web agents. Compared to developing better reasoning and planning frameworks, comparatively less attention has been given to optimizing the LLMs themselves to better handle web environments. Due to the difficulty of directly generating a single target element from the raw HTML, which often contains thousands of elements, existing work mostly focuses on multistage prediction. MindAct (Deng et al., 2023) proposes a two-stage pipeline that first uses a small LMto filter the web elements and then uses a more powerful LM to select from the filtered elements in a multi-choice question answering format. Both LMs can be fine-tuned using the Mind2Web dataset. WebAgent (Gur et al., 2023) uses HTML-5 to first process the HTML and then fine-tunes a 540B Flan-UPalm to generate code for controlling web pages. More recently, AutoWebGLM (Lai et al., 2024) trains a single ChatGLM3 6B (GLM et al., 2024) using a combination of curriculum learning, reinforcement learning, and rejection sampling fine-tuning. NNetnav (Murty et al., 2024a) leverages synthetic demonstrations collected by zero-shot LLM exploration and hindsight relabeling to fine-tune open-source models into web agents. Despite the complicated training and inference procedures, these methods often underperform agents that prompt GPT-4. In contrast, our work shows that given sufficient high-quality workflow data, fine-tuning a single LLM can achieve strong performance. We note that the newly released OpenAI o1 (OpenAI, 2024c) can be viewed as a specialized agent with a complicated planning framework. Nonetheless, we show in Section 4.1 that ScribeAgent outperforms o1-preview by a large margin on our proprietary dataset. Moreover, while none of the training details for o1 have been released, our work provides valuable insights into data preprocessing and fine-tuning.\n\nBeyond the aforementioned work, there is an earlier line of research that fine-tunes LLMs for HTML inputs (Gur et al., 2022; Nakano et al., 2022; Liu et al., 2023). However, their primary application is question-answering tasks, such as answering 'could sunflowers really track the sun across the sky', and they cannot be used to generate a sequence of actions based solely on the user objective.\n\nLastly, we note that an emerging line of research has committed to developing multi-modal web agents that use screenshots along with HTML observations. Examples include CogAgent (Hong et al., 2023), SeeClick (Cheng et al., 2024), WebGUM (Furuta et al., 2024), WebVoyager (He et al., 2024), and AWA 1.5 (JaceAI, 2024). However, our current version of ScribeAgent focuses exclusively on text-based inputs due to the lack of effective visual preprocessing schemes. Thus, we do not compare with the aforementioned multi-modal methods in our experiments and leave developing multi-modal ScribeAgent as future work.\n\n## 3 METHOD\n\nIn this section, we first overview the general setup of solving web-based tasks with LLM agents. Then, we detail our proposed method to develop specialized agents from open-source LLMs.\n\n## 3.1 GENERAL SETUP\n\nWe consider solving a web-based task as a sequential decision-making process guided by a highlevel objective. For each task, the user first specifies an objective and a starting web page. Then, at every step, the agent outputs an action based on the task objective, the current web page, and the history. Formally, denote the user objective as q . The web environment is governed by a transition function T that can evolve over time. The agent is instantiated by a language model L . At each time step t , the agent observes o t produced by the environment state s t and observes the history h t = H ( o 1: t -1 , a 1: t -1 ) . It outputs an action a t = L ( q, o t , h t ) , which is executed in the environment, and the state changes correspondingly s t +1 = T ( s t , a t ) . This iterative process stops when the agent issues a stop signal, or a task termination condition is met, such as we have reached a predefined maximum number of steps.\n\nFor single-modal, text-only agents, the observation o t typically consists of the website's URL, the HTML-DOM (Object Model for HTML, which defines HTML elements and their properties, methods, and events), and potentially the accessibility tree (a representation that can be understood by assistive technologies like screen readers). Since the raw HTML-DOM is often long and contains redundant structural information, most methods employ preprocessing and pruning strategies, which could be as simple as retaining a fixed set of HTML tags and attributes or more complex ones like LLM-based element ranking and filtering (Deng et al., 2023).\n\nThe action a t emulates the keyboard and mouse operations available on web pages. The most general action space in existing work consists of element operations, such as clicking, typing, and key combination pressing; tab actions, such as opening, closing, and switching between tabs; navigation actions, such as going forward and backward in the browsing history (Zhou et al., 2024).\n\nAs discussed earlier, previous web agent work focuses on presenting useful demonstrations through h t or iteratively revising a t to improve the quality of the predicted next step. In contrast, we explore whether we can improve the model L itself by learning from a vast amount of data and incorporating more information into o t , such as the natural language description and HTML representation of a action. We detail our approach in the next section.\n\n## 3.2 SCRIBEAGENT: SPECIALIZING WEB AGENTS THROUGH FINE-TUNING\n\n## 3.2.1 COLLECTING PRODUCTION-SCALE DATA\n\nWe collected a large set of real-world, user-annotated, proprietary data through Scribe, a software that streamlines the creation of step-by-step guides for web-based tasks. Scribe allows users to record their interactions with the web through a browser extension and converts the interactions into well-annotated instructions, which can be then customized to specific business needs. The collected dataset consists of everyday workflows in common web application domains, encompassing customer relationship management (CRM) tools like HubSpot and Salesforce; productivity tools like Notion and Calendley; social platforms like Facebook and LinkedIn; shopping sites like Amazon and Shopify; and many others.\n\nEach workflow features a high-level user objective and a step-by-step documentation of the action sequence to achieve the task. The objective spans a wide range of topics, such as 'add a user in a Salesforce' or 'invite someone to manage Facebook ad accounts'. Each step contains the following information: the current web page's URL, raw HTML-DOM, a natural language description of the action performed, the type of action, and the autogenerated CSS selector to identify the action target. There are three types of actions in the dataset:\n\n- · mouse click action : click at an element\n- · keyboard sequence action : type a sequence of characters to an element\n- · keyboard combination action : press a set of keys together (e.g., hotkey like ctrl+c)\n\nNote that there is no scroll actions in our action space since all elements are already fully accessible in the captured data. This is because we capture the full DOM from a system perspective, which inherently includes the entire webpage as observed from the backend. This method differs from usercentric data collection, where only the elements within the visible browser viewport are captured.\n\nTo ensure the quality of the data, we remove workflows with invalid selectors, i.e., the selector cannot be used to locate a target element in the DOM. We also remove non-English workflows to reduce dataset complexity and enable us to explore English-only LLMs like Mistral 7B (MistralAI, 2023). The resulting dataset is at production scale: using raw data collected over a two-month period, we are able to extract workflow data from more than 250 domains and 10,000 subdomains with an average task length of 11 steps, which correspond to about 6 billion training tokens. This large-scale, high-quality, real-world dataset is unmatched in prior web agent research.\n\nSince this dataset is collected from real users and might contain sensitive and confidential information, it will not be released to the public to protect user privacy. The dataset is solely for research purposes and has been anonymized to prevent the identification of any individual.\n\n## 3.2.2 PREPROCESSING\n\nFor ScribeAgent, we consider an observation space consisting mainly of the URL and HTML-DOM. Specifically, HTML-DOM provides agents with all structural and content information about the web page that are essential for generating the next step and long-term planning. For instance, while a drop-down menu may not be visible on the website before expansion, the agent can detect the menu items from the DOM and determine whether to click and expand it. We do not use accessibility tree to develop ScribeAgent because it may lose information about the HTML elements, such as the drop-down items, and does not generalize across different browsers and devices.\n\nGiven our observation space, a subsequent problem is that the DOM can be quite long and exceed the context window of prevailing open-source LLMs. To reduce the DOM sizes, we propose a pruning algorithm that maintains the essential structure and content while eliminating redundant or disruptive elements that could hinder the LLM's understanding. Specifically, we first use the BeautifulSoup library (Richardson, 2007) to remove non-essential components such as metadata, CSS, and JavaScript. Then, we utilize a tag-attribute white list to retain useful tag level information like retaining interactive elements. Since some attribute values can contain random character sequences that do not provide useful information, we propose a novel detection method that removes the attributes with character-to-token-ratio smaller than 2, i.e., len ( s ) len ( tokenizer ( s )) < 2 , where s denotes the value string. Intuitively, if each character in a string is encoded using a separate token, it is highly likely that the string is not semantically meaningful. Lastly, we remove the comments and extra whitespaces to clean up the DOM. After pruning, we assign each tag in the HTML with a unique ID by traversing the HTML tree from bottom to top. More details about preprocessing and analysis on the tokenizer-pruning method can be found in Appendix A.1.\n\nWerestrict the action space of ScribeAgent to the three types of operations specified in Section 3.2.1. To preprocess the action sequences, we rewrite each step into five lines as follows:\n\n1. Description: Click the 'Menu' button to browse all food options\n\nAction: mouse click action\n\nNode: 832\n\nTarget: < svg class='open-hamburger-icon' node='832' role='img' >\n\nThe first line represents the current time step. The second line is the natural language description of the action, which can help LLMs to learn about the rationale behind applying a specific action. The third line is one of the three operations in the action space. The fourth line is the unique ID assigned to the target element. The last line details the HTML tag and attributes, which can be directly obtained from the processed DOM.\n\nFor the history, we consider only previous actions, omitting previous observations due to the extensive length of the DOMs. That is, h t = a 1: t -1 . Therefore, at each step, ScribeAgent will be given the task objective, URL, HTML-DOM, and all previous actions in the aforementioned five-\n\nTable 1: Performance of different LLMs fine-tuned on 1B workflow tokens on the test split of our proprietary dataset. We highlight the best results for small/medium/large models. EM is short for Exact Match.\n\n| Model                      | # Params   | Before Fine-Tuning   | Before Fine-Tuning   | After Fine-Tuning   | After Fine-Tuning   |\n|----------------------------|------------|----------------------|----------------------|---------------------|---------------------|\n|                            |            | EM(%)                | Calibrated EM (%)    | EM(%)               | Calibrated EM (%)   |\n| Mistral-7B-Instruct-v0.3   | 7B         | 3.89                 | 5.13                 | 19.92               | 26.31               |\n| Qwen2-7B-Instruct          | 7B         | 6.06                 | 7.92                 | 29.34               | 38.72               |\n| Llama-3.1-Instruct-8B      | 8B         | 1.42                 | 1.88                 | 28.34               | 37.42               |\n| Qwen2.5-14B-Instruct       | 14B        | 8.79                 | 11.60                | 31.76               | 41.89               |\n| Codestral-22B-v0.1         | 22B        | 4.53                 | 6.08                 | 31.11               | 41.25               |\n| Qwen2.5-32B-Instruct       | 32B        | 10.02                | 13.21                | 32.98               | 43.51               |\n| Mixtral-8x7B-Instruct-v0.1 | 56B-A12B   | 7.35                 | 9.82                 | 28.38               | 37.49               |\n| Qwen2-57B-A14-Instruct     | 57B-A14B   | 5.72                 | 7.51                 | 31.02               | 40.10               |\n\nline format. Its goal is to output the next action a t = L ( q, o t , a 1: t -1 ) that helps complete the task. In Appendix A.2, we provide an example of a full workflow.\n\nLastly, during our inspection, we find that 10% of the action descriptions in the dataset are not informative (e.g., 'click here'). In these cases, we use GPT-4o (OpenAI, 2024b) to regenerate the action description from screenshots. We provide the prompt as well as examples of the regenerated action descriptions in Appendix A.3.1.\n\n## 3.2.3 FINE-TUNING WITH LORA\n\nAfter preprocessing, we divide the dataset into two splits. The test set comprises of 1200 workflows with diverse objectives and domains. We use the remaining workflows as the training data to adapt LLMs via standard supervised fine-tuning. Note that for each fine-tuning example, the label is a single next-step instead of all remaining steps needed to complete the task. The agent is trained to generate all information in the five-line format described above, including the natural language description.\n\nTo reduce fine-tuning cost, we opt for the parameter efficient method LoRA (Hu et al., 2022) instead of full fine-tuning, since we have not observed significant performance gain by updating more parameters. We also follow previous work (Zhao et al., 2023; Shen et al., 2023) to fine-tune the layernorms in addition to the LoRA adapters. Based on empirical observations, we set the fine-tuning epoch to 2, effective batch size to 32, LoRA rank to 64 and α to 128. We use a cosine scheduler with 30 warmup steps and a learning rate of 1e-4.\n\n## 3.2.4 EXPLORING THE DESIGN SPACE\n\nThere are multiple design choices for ScribeAgent that might affect the prediction accuracy, finetuning cost, and inference latency. We focus on three aspects and perform detailed ablation studies to find out the optimal modeling and training configurations.\n\nPretrained LLM Selection. Intuitively, the quality of a fine-tuned web agent should be relevant to the quality of the pretained LLM. We identify two axes that are crucial to performance-model architecture and model size-and explore seven open-source LLMs spanning these axes: Llama 3.1 8B (Dubey et al., 2024), Mistral 7B (MistralAI, 2023), Mixtral 8x7B (MistralAI, 2024b), Qwen2 7B (Yang et al., 2024a), Qwen2 57B (Yang et al., 2024a), Qwen2.5 14B (Team, 2024), Qwen2.5 32B (Team, 2024), and Codestral 22B (MistralAI, 2024a). We fine-tune these models with 1 billion training tokens and evaluate their performance on the test split of the dataset we collected.\n\nGiven that many of the evaluated LLMs have a maximum context window of approximately 32K, and the processed DOM can exceed this limit, we divide the DOM sequentially into chunks that fit into the context window. For fine-tuning, we use the chunk containing the correct target, but for evaluation, we use the last chunk since the target's location is not known beforehand. When evaluating at a 32K context window, 25% of the test data do not have the correct target tag in the DOM, i.e., these tasks are unachievable. Thus, we compute two metrics for evaluation: (1)\n\nexact match (EM) measures the model's ability to select exactly the same HTML tag as the ground truth; (2) calibrated exact match (Calibrated EM, or CEM) measures the percentage of correct target predictions where the target tag was present in the truncated HTML DOM, i.e., it is EM on the set of examples where the observation contains sufficient information to complete the task. As we scale the context window, these two metrics converge.\n\nWe report the performance of different LLMs before and after fine-tuning in Table 1. Notably, for all models, specialized fine-tuning drastically increases the prediction accuracy. Both before and after fine-tuning, the Qwen family demonstrates better EM and CEM across small, medium, and large models. We observe performance gains as model size increases. For example, the calibrated EM for Qwen2 57B is higher than its 7B counterpart. Qwen2.5 32B is also better than Qwen2.5 14B. Mixtral 8x7B outperforms Mistral 7B by a large margin as well. However, fine-tuning larger models is significantly more resource-intensive-while Qwen2 7B can be fine-tuned using 8 H100 GPUs in just one day, Qwen2 57B takes over a week using the same hardware configuration. Larger models also incur longer inference times and require multiple GPUs even at a 32K context length. To balance effectiveness and efficiency, we develop two versions of ScribeAgent using Qwen2 7B and Qwen2.5 32B, respectively. The Qwen2 7B model offers an optimal balance between prediction accuracy and the costs associated with fine-tuning and inference. Meanwhile, the Qwen2.5 32B model provides stronger performance when we have sufficient computational resources.\n\nContext Window Length. We evaluate the models with 65K context window to add additional context and increase the rate of solvable tasks (Table 2). On both Qwen2 and Qwen2.5, scaling the context window from 32K to 65K leads to approximately 2% performance boost for Exact Match but approximately 2 . 5% performance drop for Calibrated Exact Match. We hypothesize that this performance degradation might be due to rotary position embedding (Su et al., 2021) and the fact that it becomes\n\nTable 2: Ablations on context window length.\n\n| Model       | Context   |   EM(%) |   CEM(%) |\n|-------------|-----------|---------|----------|\n| Qwen2 7B    | 32K       |   29.34 |    38.72 |\n| Qwen2 7B    | 65K       |   31.42 |    36.22 |\n| Qwen2.5 14B | 32K       |   31.76 |    41.89 |\n| Qwen2.5 14B | 65K       |   33.96 |    39.15 |\n| Qwen2.5 32B | 32K       |   32.98 |    43.51 |\n| Qwen2.5 32B | 65K       |   36.16 |    41.69 |\n\nharder to pick the correct target given twice as many options to choose from. Besides, we note that using 65K context window increases the inference time by approximately four times in practice.\n\nDataset Size. Lastly, we are interested in understanding the effect of fine-tuning dataset size on the agent's performance. To this end, we sample our training set without replacement into smaller subsets and fine-tune Qwen2 7B on them. Results are shown in Table 3. Plotting on a log-linear scale, we observe that there is a roughly 2% performance boost when we double our dataset size.\n\nTable 3: Ablations on dataset size. All settings are trained and evaluated with Qwen27B-Instruct and 32K context window.\n\n| # Train Tokens   |   EM(%) |   CEM(%) |\n|------------------|---------|----------|\n| 1B               |   29.34 |    38.72 |\n| 3B               |   32.65 |    43.06 |\n| 6B               |   34.96 |    46.42 |\n\nTo sum up, using our proprietary dataset, we study the effect of LLM backbone, context window, and dataset size on the agent performance. We find that (1) scaling parameter count generally improves prediction quality, but the latency and training time of large LLMs can be prohibitive; (2) using longer context window boosts model performance on EM but increases the inference time significantly; (3) training with more tokens is helpful. Based on these insights, we develop two versions of ScribeAgent: ScribeAgent-Small, based on Qwen2 7B, and ScribeAgent-Large, based on Qwen2.5 32B. Both versions are fine-tuned on the full 6B-token dataset at a 32K context window. While ScribeAgent-Large demonstrates better performance in both internal and external evaluation,s the 7B ScribeAgent-Small is cheaper to serve at inference time, particularly when compared to large-scale proprietary models.\n\n## 4 RESULTS\n\nWe evaluate ScribeAgent on three web datasets. We first consider the next-step prediction setting, where performance is evaluated only on a single next step. We show that ScribeAgent not only outperforms various general-purpose LLMs on our proprietary dataset but also achieves state-ofthe-art on the public benchmark Mind2Web (Deng et al., 2023). Then, we move to the end-to-end task completion benchmark WebArena (Zhou et al., 2024) and show that ScribeAgent augmented\n\nTable 4: ScribeAgent v.s. non-fine-tuned, general-purpose baselines on the full test set with truncation at 32K tokens.\n\n| Model             |   EM(%) |   CEM(%) |\n|-------------------|---------|----------|\n| Qwen2 7B          |    6.28 |     8.2  |\n| GPT-4o mini       |   12.6  |    13.26 |\n| GPT-4o            |   15.24 |    16.02 |\n| ScribeAgent-Small |   34.96 |    46.42 |\n| ScribeAgent-Large |   37.67 |    49.67 |\n\nFigure 2: Exact Match (EM) comparison between ScribeAgent-Small and OpenAI models across different types of websites.\n\n<!-- image -->\n\nwith GPT-4o achieves top performance among all existing agent systems. Note that we do not perform any task-specific adaptation for Mind2Web and WebArena, even when additional training data is available. This allows us to evaluate the generalization ability of ScribeAgent. Our focus is on ensuring that ScribeAgent remains versatile and robust across diverse settings, rather than optimizing for specific benchmarks.\n\n## 4.1 PROPRIETARY DATASET\n\nTo study whether specialized fine-tuning is indeed beneficial, we first compare the performance of ScribeAgent with general-purpose baselines on our proprietary test data. We consider the non-finetuned Qwen2 7B, GPT-4o, and GPT-4o mini. We use in-context demonstrations to prompt them to generate actions in the same five-line format as defined in Section 3.2.2. All OpenAI baselines in this work follow the prompt in Appendix A.3.2.\n\nResults on the full 1200 test workflows are shown in Table 4. First, we note that ScribeAgent significantly outperforms the proprietary GPT-4o and 4o mini. This shows the benefit of specialized fine-tuning over using general-purpose LLMs. Moreover, while the non-fine-tuned Qwen2 performs extremely poorly, fine-tuning with our dataset (ScribeAgent-Small) boosts its performance by nearly 6 × , which highlights the importance of domain-specific data.\n\nWe also plot the Exact Match metric for four types of commonly seen domains, including customer relationship management (CRM) tools, E-commerce platforms, productivity tools, and social platforms (Figure 2). While our agent's performance varies by domain, with a 6% gap between the best performing domain and the worst performing one, we observe that ScribeAgent consistently outperforms the general-purpose baselines across all of them.\n\nAs we were wrapping up this work, OpenAI released o1 (OpenAI, 2024c), a series of specialized models for solving complex tasks in science, coding, and math. Since it has better planning ability, we also include it in our baselines. However, we did not run the o1 models on the full test set due to cost and API call limitations. Instead, we subsample 500 workflows and compare with ScribeAgent. As shown in Table 5, o1-preview performs the best among all general-purpose baselines. However, ScribeAgent still outperforms it by a wide margin, highlighting the importance of fine-tuning on real-world web navigation data. It\n\nTable 5: Comparing ScribeAgent with OpenAI baselines on 500 test samples. All models are evaluated at a 128K context window.\n\n| Models            |   EM(%) |   CEM(%) |\n|-------------------|---------|----------|\n| o1-mini           |    17.4 |    18.32 |\n| o1-preview        |    22.6 |    23.79 |\n| GPT-4o mini       |    13.8 |    14.53 |\n| GPT-4o            |    16.6 |    17.96 |\n| ScribeAgent-Small |    47.6 |    50.11 |\n| ScribeAgent-Large |    50   |    52.6  |\n\nis important to note that ScribeAgent-Small only has 7B parameters, while ScribeAgent-Large has 32B parameters, and neither model requires additional scaling during inference. In contrast, most proprietary baselines are typically larger in size and require more compute at inference. This makes ScribeAgent a better choice in terms of accuracy, latency, and cost.\n\nTable 6: ScribeAgent achieves state-of-the-art zero-shot performance on Mind2Web. EA is short for element accuracy, AF 1 is short for action F 1 , and SR is short for success rate. We note that the three categories are based on increasing level of domain generalization difficulty. However, since we do not train on Mind2Web data, our performance is similar across different test sets. Numbers are bolded for each method category.\n\n| Method          |                              | Cross-Task   | Cross-Task   | Cross-Task   | Cross-Task      | Cross-Website   | Cross-Website   | Cross-Website   | Cross-Website   | Cross-Domain   | Cross-Domain   | Cross-Domain   | Cross-Domain    |\n|-----------------|------------------------------|--------------|--------------|--------------|-----------------|-----------------|-----------------|-----------------|-----------------|----------------|----------------|----------------|-----------------|\n|                 |                              |              | EA AF 1      |              | Step SR Task SR |                 | EA AF 1         |                 | Step SR Task SR |                | EA AF 1        |                | Step SR Task SR |\n|                 | Uses M2W Train Set           |              |              |              |                 |                 |                 |                 |                 |                |                |                |                 |\n|                 | MindAct (Flan-T5 B )         | 43.6         | 76.8         | 41.0         | 4.0             | 32.1            | 67.6            | 29.5            | 1.7             | 33.9           | 67.3           | 31.6           | 1.6             |\n|                 | MindAct (Flan-T5 L )         | 53.4         | 75.7         | 50.3         | 7.1             | 39.2            | 67.1            | 35.3            | 1.1             | 39.7           | 67.2           | 37.3           | 2.7             |\n|                 | MindAct (Flan-T5 XL )        | 55.1         | 75.7         | 52.0         | 5.2             | 42.0            | 65.2            | 38.9            | 5.1             | 42.1           | 66.5           | 39.6           | 2.9             |\n|                 | AWM-offline (GPT-4)          | 50.6         | 57.3         | 45.1         | 4.8             | 41.4            | 46.2            | 33.7            | 2.3             | 36.4           | 41.6           | 32.6           | 0.7             |\n| Multi- Stage QA | HTML-T5-XL                   | 60.6         | 81.7         | 57.8         | 10.3            | 47.6            | 71.9            | 42.9            | 5.6             | 50.2           | 74.9           | 48.3           | 5.1             |\n| Multi- Stage QA | Zero-Shot                    |              |              |              |                 |                 |                 |                 |                 |                |                |                |                 |\n| Multi- Stage QA | MindAct (GPT-4)              | 41.6         | 60.6         | 36.2         | 2.0             | 35.8            | 51.1            | 30.1            | 2.0             | 21.6           | 52.8           | 18.6           | 1.0             |\n| Multi- Stage QA | AWM-online (GPT-4)           | 50.0         | 56.4         | 43.6         | 4.0             | 42.1            | 45.1            | 33.9            | 1.6             | 40.9           | 46.3           | 35.5           | 1.7             |\n| Multi- Stage QA | ScribeAgent Small (Ours)     | 42.6         | 50.1         | 39.7         | 0               | 44.9            | 50.1            | 41.6            | 0.6             | 44.1           | 51.4           | 41.4           | 0               |\n| Multi- Stage QA | ScribeAgent Large (Ours)     | 53.5         | 52.9         | 51.2         | 0               | 53.4            | 52.8            | 51.3            | 2.3             | 53.3           | 54.7           | 51.2           | 0               |\n| Direct          | Uses M2W Train Set Flan-T5 B | 20.2         | 52.0         | 17.5         | 0               | 13.9            | 44.7            | 11.0            | 0               | 14.2           | 44.7           | 11.9           | 0.4             |\n| Generation      | Synapse (GPT-3.5)            | 34.0         | -            | 30.6         | 2.4             | 29.1            | -               | 24.2            | 0.6             | 29.6           | -              | 26.4           | 1.5             |\n| Generation      | Zero-Shot                    |              |              |              |                 |                 |                 |                 |                 |                |                |                |                 |\n| Generation      | ScribeAgent Small (Ours)     | 28.6         | 50.1         | 26.8         | 0               | 27.6            | 50.1            | 25.6            | 0               | 32.0           | 51.4           | 29.9           | 0               |\n| Generation      | ScribeAgent Large (Ours)     | 38.0         | 52.9         | 35.6         | 0               | 34.1            | 52.7            | 32.5            | 0               | 39.4           | 54.7           | 37.3           | 0               |\n\n## 4.2 MIND2WEB\n\nMind2Web (Deng et al., 2023) is a text-based dataset for assessing the navigation ability of web agents across different tasks, websites, and domains. Each task features a human demonstration of a real-world workflow, such as booking a hotel on Airbnb. At each step, the agent is asked to predict a single action, consisting of an operation and the target element. Performance is measured by element accuracy, which checks if the correct target is selected; action F1 score, which measures operation correctness like text input; step success rate, which evaluates whether both the target element and the operation are correct; and task success rate, indicating all steps are correct.\n\nThe original Mind2Web benchmark reports two sets of baselines: (1) multi-stage, multi-choice question-answering agents (i.e., the MindAct family) first use a pretrained element-ranking model to filter out 50 candidate elements from the full DOM and then use a separate LLM to recursively select an action from five candidates in a multi-choice question-answering (QA) fashion until one action is chosen; (2) a single-stage, generation-based agent (i.e., fine-tuned Flan-T5 B ) directly generates the operation and the target based on the full DOM. The multi-stage baselines generally show higher metrics than direct generation models, as the element selection process effectively filters out noise, simplifying the task.\n\nBeyond these baselines, we also consider recent published work such as AWM (Wang et al., 2024b), Synapse (Zheng et al., 2024), and fine-tuned HTML-T5 (Gur et al., 2023). AutoWebGLM (Lai et al., 2024) reports only step success rate among all four metrics. While the reported numbers are high, it uses a different and possibly more favorable evaluation procedure, so we do not compare against it. For both single-stage and multi-stage settings, we further categorize the baselines into those leveraging the Mind2Web training data for fine-tuning or in-context demonstrations and zeroshot methods. As stated earlier, we do not fine-tune our agents because our goal is to test the agent's out-of-distribution generalization abilities.\n\nWe evaluate ScribeAgent on both multi-stage QA and direct generation. For the multi-stage setting, we first use the pretrained Mind2Web element-ranker to obtain the element ranking. Then, given the output of ScribeAgent, we traverse the sorted list of HTML elements from top to bottom, and stop when the agent's generated HTML element is a subchild of the element. We then replace ScribeAgent's prediction by the element. For direct generation, we simply compare the output of our agent to the ground truth action and target.\n\nWe report results following the evaluation procedure specified in the Mind2Web repository in Table 6. For the multi-stage setting, ScribeAgent-Large achieves the best overall zero-shot performance. Our element accuracy and step success rate metrics are also competitive with the best\n\nfine-tuned baseline, HTML-T5-XL, on cross-website and cross-domain tasks. However, our task success rates are not satisfactory, which is mainly due to the distribution differences between our training data and the Mind2Web data. Upon inspection, we find that the primary failure cases of our models are (1) predicting the subchild element of the ground truth instead of the ground truth; (2) predicting another element with identical function but is different from the ground truth; and (3) our agent tends to decompose type actions into click followed by type actions. In many cases, we actually correctly predict the action description. These situations can be addressed by improving the evaluation procedure, which we discuss later in this section and in Appendix A.4.2.\n\nAs for direct generation, ScribeAgent-Large outperforms all existing baselines. Our step success rates are 2-3 times higher than those achieved by the fine-tuned Flan-T5 and show an improvement of 5-10% over Synapse, which utilizes GPT-3.5. We attribute ScribeAgent's strong performance to the diversity and high quality of the workflows in our dataset. Relatedly, the three test sets (Cross-Task, Cross-Website, Cross-Domain) are designed to capture different degrees of domain generalization difficulty. Since we do not train on Mind2Web data, the performance of ScribeAgent is similar across all three test sets.\n\nAs mentioned earlier, we observe several limitations in the Mind2Web evaluation that underestimate our agent's performance. First, the evaluation strictly compares element IDs, where only the outermost tags are labeled as ground truths, disregarding the subchildren. This leads to inaccuracies when our agent selects functionally identical but hierarchically deeper elements, which are marked as incorrect. Second, for direct generation, ScribeAgent might select a functionally identical element that is located in a different part of the website (e.g., consider clicking on the next page button vs. clicking on the page number). Lastly, there is a notable discrepancy between the distribution of our training data and the synthetic trajectories of Mind2Web. For instance, Mind2Web expects immediate type actions for textarea or input tags, whereas our model first clicks before typing.\n\nTo better reflect ScribeAgent's capabilities, we refine the evaluation method by relaxing the labels to include subchildren of the ground truths. For direct generation, we also introduce an element attribute matching step that compares not only the element IDs but also tag and text attributes. The results with refined evaluation is shown in Appendix A.4.2. We observe an average of 8% increase in task success rate and element accuracy for ScribeAgent, showing the need for enhancing evaluation of text-based benchmarks. It is worth noting again that results in Table 6 follow the original evaluation procedures to ensure fair comparison with established baselines.\n\nWhile the Mind2Web results are promising, we note that another limitation of static, text-based benchmark is that the ground truth evaluation does not account for different action sequences that could reach the same goal. For instance, to book a flight, one can first enter the destination or first choose the departure date, but the ground truth trajectory only accounts for one possibility. Considering this, we also evaluated ScribeAgent on a dynamic benchmark WebArena (Zhou et al., 2024).\n\n## 4.3 END-TO-END TASK EXECUTION ON WEBARENA\n\nWebArena (Zhou et al., 2024) features 812 web navigation tasks across five domains: E-commerce (OneStopShop), social forums (Reddit), software development (GitLab), content management (CMS), and online map (OpenStreetMap). Unlike the static Mind2Web, it implements a dynamic environment for agents to interact with and allows for assessing the functional accuracy of action sequences. Since the WebArena environment is implemented to accept only target element IDs specified in the accessibility tree, whereas ScribeAgent operates on DOM and outputs targets in HTML, we employ GPT-4o to map between the different representations.\n\nMore generally, we tackle end-to-end task solving by developing a multi-agent system that utilizes GPT-4o to simulate user interactions with ScribeAgent. Our system contains four stages: (1) objective refinement: user adds details about the task objective to help complete the task; (2) action generation: based on the current website and action history, agent outputs an action suggestion; (3) action execution: user executes the suggested action, e.g., clicking a button; (4) completeness evaluation: user observes the current state and decides whether the task is completed.\n\nWe apply the above pipeline to solve the WebArena tasks. In stage 3, GPT-4o maps the agent's output in HTML to the accessibility tree format, which is then processed by the WebArena environ-\n\nTable 7: Task success rates (SR) on WebArena and score breakdown on five web domains. ScribeAgent consistently outperforms considered text-only baselines, often improving the previousbest results by more than 5%. Note that we only test ScribeAgent-Small due to the large number of tasks in WebArena and the evaluation costs associated with the multi-agent system.Table 8: We replace ScribeAgent with GPT-4o in our four-stage pipeline to study how much ScribeAgent contributes to the performance. The success rates drop significantly for all domains.\n\n| Method                      | LLM                        |   Total SR | Shopping   |      | CMS Reddit   | GitLab   | Maps   |\n|-----------------------------|----------------------------|------------|------------|------|--------------|----------|--------|\n| AutoWebGLM                  | ChatGLM3 6B                |       18.2 | -          | -    | -            | -        | -      |\n| NNetnav                     | Llama 3 8B Instruct        |        7.2 | 7.4        | 4.2  | 0            | 0        | 28.5   |\n| AutoEval                    | GPT-4                      |       20.2 | 25.5       | 18.1 | 25.4         | 28.6     | 31.9   |\n| BrowserGym                  | GPT-4                      |       23.5 | -          | -    | -            | -        | -      |\n| BrowserGym axtree           | GPT-4                      |       15   | 17.2       | 14.8 | 20.2         | 19.0     | 25.5   |\n| SteP                        | GPT-4                      |       33   | 37.0       | 24.0 | 59.0         | 32.0     | 30.0   |\n| AWM                         | GPT-4                      |       35.5 | 30.8       | 29.1 | 50.9         | 31.8     | 43.3   |\n| Tree Search                 | GPT-4o                     |       19.2 | -          | -    | -            | -        | -      |\n| WebPilot                    | GPT-4o                     |       37.2 | 36.9       | 24.7 | 65.1         | 39.4     | 33.9   |\n| Broswing+API Hybrid Agent   | GPT-4o                     |       35.8 | 25.7       | 41.2 | 28.3         | 44.4     | 45.9   |\n| AgentOccam with Judge       | GPT-4-Turbo                |       45.7 | 43.3       | 46.2 | 67.0         | 38.9     | 52.3   |\n| Multi-Agent System ( Ours ) | ScribeAgent-Small + GPT-4o |       51.3 | 48.1       | 35.5 | 70.2         | 58.8     | 51.9   |\n| Multi-Agent System ( Ours ) | ScribeAgent-Large + GPT-4o |       53   | 45.8       | 37.9 | 73.7         | 59.7     | 56.3   |\n\n| Method       | LLM                        |   Total SR |   Shopping |   CMS |   Reddit |   GitLab |   Maps |\n|--------------|----------------------------|------------|------------|-------|----------|----------|--------|\n| Single-Agent | GPT-4o                     |       34.2 |       31.9 |  21.3 |     44.7 |     38.2 |   42.6 |\n| Multi-Agent  | ScribeAgent-Small + GPT-4o |       51.3 |       48.1 |  35.5 |     70.2 |     58.8 |   51.9 |\n\nment. To further improve performance, we allow ScribeAgent to generate multiple actions in stage 2 and select the one with the highest confidence using majority vote and GPT-4o analysis. More details about evaluating ScribeAgent on WebArena can be found in Appendix A.5.\n\nWe compare our performance with all top-performing, text-only agents on the WebArena leaderboard. We note that we do not include Autonomous Web Agent (AWA) 1.5 (JaceAI, 2024) as a baseline because it uses a proprietary system to parse the HTML-DOM and web screenshots, rather than building from the WebArena GitHub. This allows them to have richer observations and bypass the accessibility tree action mapping step. In contrast, ScribeAgent is single-modal, text-only, and we stick to the original WebArena implementation. That said, AWA 1.5 employs more advanced reasoning, planning, and progress tracking techniques and is the only agent system with a higher average task success rate than ours.\n\nThe results are shown in Table 7. Compared with existing text-only baselines, ScribeAgent augmented with GPT-4o obtains the highest task success rate in 4 of 5 categories, leading to 7.3% performance improvements in total success rate over the previous-best GPT-4-Turbo-based AgentOccam (Yang et al., 2024b). In particular, on Reddit and GitLab tasks where the domains are more realistic and thus closer to the ones in our training data, ScribeAgent demonstrates stronger generalization ability and higher task success rates than in other domains. Despite known issues with combobox selection and the absence of scroll actions in our training data, our agent effectively navigates these challenges through strategic keyboard actions. More details are provided in Appendix A.5.2.\n\nTo better understand the contribution of ScribeAgent to the multi-agent system, we perform an ablation study that leverages GPT-4o for all four stages of the proposed pipeline. Given the large number of tasks in WebArena and the evaluation costs associated with using a multi-agent system, we perform our ablation studies using ScribeAgent-Small. As shown in Table 8, using ScribeAgent consistently outperforms only using GPT-4o, and the GPT-4o-only setting is less effective than existing agents like WebPilot. This shows that our strong performance on WebArena can be mainly attributed to the action generation process of ScribeAgent. Apart from getting better results, the multi-agent system is cheaper than using GPT-4o alone, as calling ScribeAgent to generate a next action incurs negligible cost as it is served locally.\n\nTable 9: Task success rates on a subset of WebArena. The numbers after the domains indicate the number of tasks considered. All models are used along with GPT-4o to formulate the multi-agent system. We see that the general trends agree with what we found on our proprietary dataset.\n\n| Agent Backbone # Train Tokens Total SR (158) Shopping (36) CMS (39) Reddit (24) GitLab (33) Maps (26)   |    |      |      |      |      |      |      |\n|---------------------------------------------------------------------------------------------------------|----|------|------|------|------|------|------|\n| Mistral 7B                                                                                              | 1B | 41.8 | 41.7 | 30.8 | 50   | 42.4 | 42.3 |\n| Qwen2 7B                                                                                                | 1B | 44.3 | 52.8 | 33.3 | 50   | 48.5 | 42.3 |\n| Qwen2 7B                                                                                                | 3B | 47.5 | 55.6 | 33.3 | 58.3 | 48.5 | 46.2 |\n| Qwen2 7B                                                                                                | 6B | 55   | 58.3 | 41   | 70.8 | 63.6 | 46.2 |\n\nWe also use WebArena to verify the signals observed in our proprietary test data. To do so, we randomly select a subset of 158 WebArena tasks with non-overlapping objective templates and run ablation studies following the ones presented in Section 3.2.4 to study the effect of LLM backbones and the number of training tokens. As shown in Table 9, on all domains, Qwen2 7B outperforms Mistral 7B, and the task success rate increases as the number of training tokens increases. These trends suggest that improvements on our proprietary dataset lead to even greater improvements on WebArena, further highlighting the advantages of fine-tuning web agents with large-scale datasets.\n\n## 5 CONCLUSION\n\nIn this work, we explore how fine-tuning open-source LLMs with high-quality real-world workflow data can benefit developing specialized web agents. We present ScribeAgent, which consistently outperforms existing methods that prompt proprietary models in various evaluation settings and benchmarks. We also provide empirical insights into data processing and model fine-tuning.\n\nLimitations and Future Work. The long-context nature of DOMs presents great challenges in adapting LLMs. In the short term, we aim to enable ScribeAgent to compare and reason over multiple DOM chunks so that its observation is always complete. This might require integrating a memory component, which could also aid in maintaining context or state across interactions to improve multi-step reasoning. Besides, we currently do not incorporate planning into ScribeAgent, so its output will be directly used as the next action. However, adding better action selection strategies such as Monte Carlo Tree Search (MCTS) could potentially facilitate online planning and exploration, further improving the agent's decision-making processes in complex scenarios. In the long run, we aim to expand ScribeAgent's capabilities to handle multi-modal inputs and multilingual content. This would significantly broaden its applicability across different linguistic and visual contexts, making it more versatile and robust in real-world web environments.\n\n## ACKNOWLEDGMENT\n\nWe would like to thank the team at Scribe for their invaluable help and guidance throughout the project. Special thanks go to Thomas Kao for his assistance during the data collection and curation process. We would also like to thank Wayne Chi for helpful discussions that improved the work. This work was supported in part by the National Science Foundation grants IIS1705121, IIS1838017, IIS2046613, IIS2112471, and funding from Meta, Morgan Stanley, Amazon, Google, and Scribe. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of any of these funding agencies.\n\n## REFERENCES\n\nKanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. SeeClick: Harnessing GUI grounding for advanced visual GUI agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 9313-9332, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.505 .\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022. URL https://arxiv.org/abs/2210.11416 .\n\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2023. URL https: //openreview.net/forum?id=kiYqbO3wqw .\n\nAlexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H Laradji, Manuel Del Verme, Tom Marty, L'eo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, et al. Workarena: How capable are web agents at solving common knowledge work tasks? arXiv preprint arXiv:2403.07718 , 2024.\n\nAbhimanyu Dubey, ..., and Zhiwei Zhao. The llama 3 herd of models, 2024. URL https:// arxiv.org/abs/2407.21783 .\n\nYao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, and Honglak Lee. Autoguide: Automated generation and selection of state-aware guidelines for large language model agents. arXiv preprint arXiv:2403.08978 , 2024.\n\nHiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models, 2024. URL https://arxiv.org/abs/2305.11854 .\n\nTeam GLM, ..., and Zihan Wang. Chatglm: A family of large language models from glm-130b to glm-4 all tools, 2024.\n\nIzzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. Understanding html with large language models. ArXiv , abs/2210.03945, 2022. URL https://api.semanticscholar. org/CorpusID:252780086 .\n\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis. ArXiv , abs/2307.12856, 2023. URL https://api.semanticscholar. org/CorpusID:260126067 .\n\nHongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models, 2024. URL https://arxiv.org/abs/2401.13919 .\n\nWenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents, 2023.\n\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations , 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9 .\n\n| JaceAI. Awa 1.5 achieves                                                                                                                                                                                                                                                                                                | breakthrough we-                                                                                                                                                                           |                                                                                                                                     |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|\n| barena benchmark, 2024.                                                                                                                                                                                                                                                                                                 | URL awa-1-5-achieves-breakthrough-performance-on-webarena-benchmark .                                                                                                                      | performance on https://www.jace.ai/post/                                                                                            |\n| model agents. arXiv preprint arXiv:2407.01476                                                                                                                                                                                                                                                                           |                                                                                                                                                                                            | Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language                                      |\n| Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. based web navigating agent. In Discovery and Data Mining , pp. 5295--5306, 2024. Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, | , 2024. Autowebglm: A large language model- Proceedings of the 30th ACM SIGKDD Conference on Knowledge                                                                                     |                                                                                                                                     |\n| and Jie Tang. Webglm: Towards an efficient web-enhanced question answering system with human preferences, 2023. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri with self-feedback, 2023. MistralAI. Announcing mistral 7b, 2023.                                             | Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Ma- jumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement URL | https://mistral.ai/news/                                                                                                            |\n| MistralAI. Codestral: Hello world, 2024a. codestral/ .                                                                                                                                                                                                                                                                  |                                                                                                                                                                                            | https://mistral.ai/news/                                                                                                            |\n| MistralAI. Mixtral of experts, 2024b. mixtral-of-experts/                                                                                                                                                                                                                                                               | URL                                                                                                                                                                                        |                                                                                                                                     |\n| . abs/2410.02907 .                                                                                                                                                                                                                                                                                                      | URL                                                                                                                                                                                        | https://mistral.ai/news/                                                                                                            |\n|                                                                                                                                                                                                                                                                                                                         | demonstrations for web agents without a demonstrator, 2024a. URL                                                                                                                           | Shikhar Murty, Dzmitry Bahdanau, and Christopher D. Manning. Nnetscape navigator: Complex https://arxiv.org/                        |\n|                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                            | Shikhar Murty, Christopher Manning, Peter Shaw, Mandar Joshi, and Kenton Lee. Bagel: Boot- arXiv preprint arXiv:2403.08140 , 2024b. |\n| strapping agents by guiding exploration with language.                                                                                                                                                                                                                                                                  |                                                                                                                                                                                            |                                                                                                                                     |\n|                                                                                                                                                                                                                                                                                                                         | URL                                                                                                                                                                                        |                                                                                                                                     |\n| https://arxiv. .                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                            |                                                                                                                                     |\n| Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,                                                                                                                           |                                                                                                                                                                                            |                                                                                                                                     |\n| Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:                                                                                                                                                                                                                              |                                                                                                                                                                                            |                                                                                                                                     |\n| Browser-assisted question-answering with human feedback, 2022. org/abs/2112.09332                                                                                                                                                                                                                                       |                                                                                                                                                                                            |                                                                                                                                     |\n\nOpenAI. Gpt-4 technical report, 2024a. URL https://arxiv.org/abs/2303.08774 .\n\nOpenAI. Gpt-4o, 2024b. URL https://openai.com/index/hello-gpt-4o/ .\n\nOpenAI. Introducing openai o1, 2024c. URL https://openai.com/o1/ .\n\nTianyue Ou, Frank F. Xu, Aman Madaan, Jiarui Liu, Robert Lo, Abishek Sridhar, Sudipta Sengupta, Dan Roth, Graham Neubig, and Shuyan Zhou. Synatra: Turning indirect knowledge into direct demonstrations for digital agents at scale, 2024. URL https://arxiv.org/abs/2409. 15637 .\n\nJiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474 , 2024.\n\nLeonard Richardson. Beautiful soup documentation. April , 2007.\n\nNicholas Roberts, Samuel Guo, Cong Xu, Ameet Talwalkar, David Lander, Lvfang Tao, Linhang Cai, Shuaicheng Niu, Jianyu Heng, Hongyang Qin, Minwen Deng, Johannes Hog, Alexander Pfefferle, Sushil Ammanaghatta Shivakumar, Arjun Krishnakumar, Yubo Wang, Rhea Sanjay Sukthanker, Frank Hutter, Euxhen Hasanaj, Tien-Dung Le, Mikhail Khodak, Yuriy Nevmyvaka,\n\n| Kashif Rasul, Frederic Sala, Anderson Schneider, Junhong Shen, and Evan R. Sparks. Au- toml decathlon: Diverse tasks, modern methods, and efficiency at scale. In Neural Information Processing Systems , 2021. URL https://api.semanticscholar.org/CorpusID:                                                                                                                                                       |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Baptiste Rozi'ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi                                                                                                                                                                                                                                                                                                                 |\n| Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J'er'emy Rapin, Artyom Kozhevnikov, Ivan Ev- timov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D'efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024. URL https://arxiv.org/abs/2308.12950 . |\n| Junhong Shen, Mikhail Khodak, and Ameet Talwalkar. Efficient architecture search for diverse tasks. In Advances in Neural Information Processing Systems (NeurIPS) , 2022.                                                                                                                                                                                                                                          |\n| Junhong Shen, Liam Li, Lucio M. Dery, Corey Staten, Mikhail Khodak, Graham Neubig, and Ameet Talwalkar. Cross-modal fine-tuning: align then refine. In Proceedings of the 40th International Conference on Machine Learning , 2023.                                                                                                                                                                                 |\n| Junhong Shen, Tanya Marwah, and Ameet Talwalkar. Ups: Towards foundation models for pde solving via cross-modal adaptation. arXiv preprint arXiv:2403.07187 , 2024a. Junhong Shen, Neil Tenenholtz, James Brian Hall, David Alvarez-Melis, and Nicolo Fusi. Tag-llm:                                                                                                                                                |\n| Repurposing general-purpose llms for specialized domains, 2024b. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and                                                                                                                                                                                                                                                              |\n| Paloma Sodhi, S. R. K. Branavan, Yoav Artzi, and Ryan McDonald. Step: Stacked llm policies for web actions. In Conference on Language Modeling (COLM) , 2024. URL https://arxiv. org/abs/2310.03720 . Yueqi Song, Frank Xu, Shuyan Zhou, and Graham Neubig. Beyond browsing: Api-based web                                                                                                                          |\n| agents, 2024. URL .                                                                                                                                                                                                                                                                                                                                                                                                 |\n| https://arxiv.org/abs/2410.16464 Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021.                                                                                                                                                                                                                                                    |\n| Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive plan- ning from feedback with language models. In Thirty-seventh Conference on Neural Information Processing Systems , 2023. URL https://openreview.net/forum?id=rnKgbKmelt . Qwen Team. Qwen2.5: A party of foundation models, September 2024. URL                                                                          |\n| https://qwenlm.                                                                                                                                                                                                                                                                                                                                                                                                     |\n| , 2022.                                                                                                                                                                                                                                                                                                                                                                                                             |\n| Renbo Tu, Nicholas Roberts, Mikhail Khodak, Junhong Shen, Frederic Sala, and Ameet Talwalkar. NAS-bench-360: Benchmarking neural architecture search on diverse tasks. In Advances in Neu- ral Information Processing Systems (NeurIPS) Datasets and Benchmarks Track Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,                                                      |\n| and Anima Anandkumar. Voyager: An open-ended embodied agent with large language mod- els. Transactions on Machine Learning Research , 2024a. ISSN 2835-8856. URL https: //openreview.net/forum?id=ehfRiF0R3a .                                                                                                                                                                                                      |\n| Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. arXiv                                                                                                                                                                                                                                                                                                                             |\n| An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jin-                                                                                                                                              |\n\nRunji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024a. URL https://arxiv.org/abs/2407.10671 .\n\n- Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, and Huzefa Rangwala. Agentoccam: A simple yet strong baseline for llm-based web agents, 2024b. URL https://arxiv.org/abs/2410.13825 .\n- Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. ArXiv , abs/2210.03629, 2022. URL https://api.semanticscholar.org/CorpusID:252762395 .\n\nYao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. Webpilot: A versatile and autonomous multi-agent system for web task execution with strategic exploration, 2024. URL https://arxiv.org/abs/2408.15978 .\n\nBingchen Zhao, Haoqin Tu, Chen Wei, Jieru Mei, and Cihang Xie. Tuning layernorm in attention: Towards efficient multi-modal llm finetuning, 2023. URL https://arxiv.org/abs/ 2312.11420 .\n\nLongtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting with memory for computer control. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id=Pc8AU1aF5e .\n\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id= oKn9c6ytLx .\n\n## A APPENDIX\n\n## A.1 PREPROCESSING\n\n## A.1.1 PRUNING PIPELINE\n\nThe code for preprocessing, chunking the DOM for fine-tuning, fine-tuning, and inference can be found in our GitHub.\n\n## A.1.2 TOKENIZER PRUNING\n\nIn this section, we provide more details on the tokenizer-based detection method to remove random character strings. The rationale behind our approach is based on the observation that typical English words consist of more than two characters. Assuming the token count is t and the character count is s , this means that when t = 1 , s ≥ 2 , leading to s t ≥ 2 . By setting the pruning threshold to 2 and removing tag attributes with s t < 2 , we aim to eliminate strings composed solely of single-character tokens, which are likely to be nonsensical.\n\nIn our actual implementation, we employ this technique only for tag attributes with s > 32 , being more lenient for shorter attributes. To show that this tokenizer pruning strategy is effective and to study the performance across different tokenizers and pruning thresholds, we perform the following experiments.\n\nWe take three tokenizers from different models: Qwen2-7B-Instruct, Mistral-7B-Instruct-v0.3, and Meta-Llama-3-8B. For each tokenizer, we vary the pruning thresholds across a set of values: { 1 . 5 , 1 . 75 , 2 , 2 . 25 , 2 . 5 } . Note that it is meaningless to study overly small thresholds (e.g., it is impossible to have s t < 1 ) or overly large thresholds (e.g., s t < 3 could result in the loss of meaningful attributes, as many English words contain three letters). We randomly sample 1000 DOMs from our proprietary test dataset, apply our standard pruning pipeline followed by tokenizer pruning, and then perform three analysis:\n\n- · False positives: we use the Python enchant library to detect if there are meanful English words within the pruned strings. Note that even though these are actual words, many of them are related to DOM structure and can be safely ignored. Still, we count them as false positives since the tokenizer method is designed to remove random character strings.\n- · Average s and t for the entire DOM before and after tokenizer pruning: this is for understanding the reduction in content length.\n- · Lastly, we sort tags and attributes by the frequency of being pruned to identify patterns.\n\nTable 10: Tokenizer pruning analysis.\n\n| Tokenizer                | Prune Threshold   | False Positive (%) ↓   | Before Pruning (K) s   | Before Pruning (K) s   | After Pruning (K)   | After Pruning (K)   | After Pruning (K)   |\n|--------------------------|-------------------|------------------------|------------------------|------------------------|---------------------|---------------------|---------------------|\n|                          |                   |                        | s                      | t                      |                     | t                   | ∆ t                 |\n| Qwen2-7B-Instruct        | 1.5               | 0.025                  |                        |                        | 221.4               | 77.11               | 2.03                |\n|                          | 1.75              | 0.013                  |                        |                        | 217.3               | 74.67               | 4.47                |\n|                          | 2                 | 0.18                   | 224.3                  | 79.14                  | 215.7               | 73.89               | 5.21                |\n|                          | 2.25              | 0.36                   |                        |                        | 213.9               | 73.13               | 6.01                |\n|                          | 2.5               | 0.38                   |                        |                        | 210.0               | 71.63               | 7.51                |\n| Mistral-7B-Instruct-v0.3 | 1.5               | 0.012                  |                        |                        | 219.5               | 87.10               | 3.44                |\n| Mistral-7B-Instruct-v0.3 | 1.75              | 0.18                   |                        |                        | 216.1               | 85.07               | 5.47                |\n| Mistral-7B-Instruct-v0.3 | 2                 | 0.44                   | 224.3                  | 90.54                  | 212.7               | 83.40               | 7.14                |\n| Mistral-7B-Instruct-v0.3 | 2.25              | 0.49                   |                        |                        | 205.3               | 80.20               | 10.34               |\n| Mistral-7B-Instruct-v0.3 | 2.5               | 11.28                  |                        |                        | 190.3               | 74.44               | 16.10               |\n| Meta-Llama-3-8B          | 1.5               | 0.0097                 |                        |                        | 223.1               | 70.60               | 0.84                |\n| Meta-Llama-3-8B          | 1.75              | 0.012                  |                        |                        | 218.3               | 67.85               | 3.59                |\n| Meta-Llama-3-8B          | 2                 | 0.035                  | 224.3                  | 71.44                  | 216.8               | 67.09               | 3.43                |\n| Meta-Llama-3-8B          | 2.25              | 0.043                  |                        |                        | 215.2               | 66.41               | 5.03                |\n| Meta-Llama-3-8B          | 2.5               | 0.10                   |                        |                        | 212.7               | 65.46               | 5.98                |\n\nAs shown in Table 10, there is a clear trade-off between precision and context reduction: greater reductions in content length tend to result in higher false positive rates. While different tokenizers\n\nexhibit varying sensitivities to the pruning thresholds, a threshold of 2 achieves the most balanced trade-off, which aligns with our intuition. We then list the top-5 tag-attribute pairs most frequently pruned under threshold 2 along with their pruning counts:\n\n- · Qwen: ('div', 'class'): 3188, ('span', 'class'): 11426, ('a', 'href'): 8802, ('button', 'class'): 6844, ('i', 'class'): 5010\n- · Mistral: ('div', 'class'): 5288, ('span', 'class'): 15824, ('a', 'href'): 12948, ('button', 'class'): 7998, ('svg', 'class'): 5871\n- · Llama: ('div', 'class'): 29559, ('span', 'class'): 8823,('button', 'class'): 5889, ('i', 'class'): 4608, ('svg', 'class'): 2577\n\nAttributes such as 'class' often contain random character strings and are frequently pruned. However, we observe differences in how tokenizers handle the href attribute: both Qwen and Mistral tokenizers tend to prune it away, whereas the Llama tokenizer preserves it, indicating its better capability in tokenizing URLs. Although we currently use the Qwen tokenizer in our preprocessing pipeline to align with the backbone model of ScribeAgent, the Llama tokenizer can be a compelling alternative for future consideration since it is better at recognizing URLs and producing shorter token sequences. In general, we believe developing specialized models can be important to achieve strong results, as evidenced in prior works (Shen et al., 2024a; Tu et al., 2022; Shen et al., 2022; Roberts et al., 2021).\n\n## A.2 EXAMPLE PROMPT AND LABEL FOR SCRIBEAGENT\n\n```\nObjective: Grant delegation access to another user in Gmail settings. URL: https://mail.google.com/mail/u/0/ Observation: {processed dom} Step-by-step guide: 1. Description: Click \"See all settings\" Action: mouse\\_click\\_action Node: 254 Target: <button class=\"Tj\" node=\"254\"> 2. Description: Click \"Accounts\" Action: mouse\\_click\\_action Node: 2625 Target: <a class=\"f0 LJOhwe\" href=\"https://mail.google.com/mail/u/0/? tab=#settings/accounts\" node=\"2625\" role=\"tab\"> 3. Description: Click \"Add another account\" Action: mouse\\_click\\_action Node: 1215 Target: <span class=\"LJOhwe sA\" id=\":kp\" node=\"1215\" role=\"link\">\n```\n\n## A.3 OPENAI PROMPTS\n\n## A.3.1 DATA PREPARATION\n\nBelow shows the prompt to generate step descriptions.\n\nYou are navigating a webpage to achieve an objective. Given the objective, a list of the previous actions, the current action, and a screenshot of the current action on the webpage. The objective and previous steps are only here to ground the current step, the current action and its screenshot are the most useful to your task. Give me a concise description of the current action being done on the webpage. You should look at the part of the webpage with the red circle, this is where the user clicked for the current action. Describe this action\n\nand ensure your response is in the same format, concise, coherent. Use any relevant information in the image to ground the action description. Your response should NOT use any json or markdown formatting. The response should be a single sentence that starts with an action verb. For example, 'Click on the 'SUBMIT' button.'\n\nRegenerated Action Descriptions. We provide a few examples of generated action descriptions using GPT-4o.\n\n- · 'Click on the Submit button.'\n- · 'Type in the name of the item.'\n- · 'Double-click on the highlighted text.'\n\n## A.3.2 PROPRIETARY BENCHMARK BASELINES\n\nBelow shows the prompt for all OpenAI baselines. The text is the prepend for every input to which we append the task input with the corresponding objective, URL, DOM, and action history.\n\nYou are an autonomous intelligent agent tasked with solving web-based tasks. These tasks will be accomplished through the use of specific actions you can issue.\n\nHere's the information you'll have:\n\n- - The user's objective: This is the task you're trying to complete.\n- - The current web page's URL: This is the page you're currently navigating.\n- - Part of the current web page's HTML: Each element is assigned in descending order with an unique ID, denoted by the attribute node\\\".\n\nThe actions you can perform include:\n\n- - mouse\\_click\\_action: click\n- - keyboard\\_sequence\\_action: type a sequence of characters\n- - keyboard\\_combination\\_action: press a set of keys together\n\n(e.g., hotkey like ctrl+c) You will generate a step-by-step guide to complete the task based on the given information. You will only produce a SINGLE next step. Do NOT use additional punctuation, or any markdown formatting. The output should be in the following format: Description: Click Users\\\" Action: mouse\\_click\\_action Node: 93\n\nTarget: <a node=\\\"93\\\" class=slds-tree\\_\\_item-label\\\"> Now complete the following task by generating the next step. {task input}\n\n## A.4 MIND2WEB EXPERIMENT DETAILS\n\n## A.4.1 PREPROCESSING\n\nData and Label Conversion. To apply ScribeAgent to Mind2Web data, we first re-process the provided DOM using the procedure detailed in Section 3.2.2. We store a map between our node ID and the backend ID given in the dataset. Then, we transform the history action provided in the dataset to our 5-line format. After ScribeAgent generates the next step, we check the backend ID of the provided label and map it to the node ID in our processed DOM. We then compare this label with the target node ID generated by ScribeAgent. We provide the code for the DOM processing and label conversion process in the supplementary material and will release them later.\n\nDOMChunkingandAction Generation. Whenthe DOM length exceeds the 32K context window, we chunk the DOM sequentially and run the prediction workflow on each piece. For each piece of DOM, we call ScribeAgent five times to obtain five valid actions. We then aggregate all possible actions and select the one with the highest number of appearances. We use the following generation configuration: do sample=True, top p=0.95, temperature=0.6.\n\nTable 11: We also refine the evaluation procedure to better reflect ScribeAgent's capacity.\n\n| Models                   | Eval                                |                                     | Cross-Task   |   Cross-Task | Cross-Task      | Cross-Task   | Cross-Website   | Cross-Website   | Cross-Website   | Cross-Website   | Cross-Domain   | Cross-Domain   | Cross-Domain   | Cross-Domain    |\n|--------------------------|-------------------------------------|-------------------------------------|--------------|--------------|-----------------|--------------|-----------------|-----------------|-----------------|-----------------|----------------|----------------|----------------|-----------------|\n|                          |                                     |                                     | EA AF        |          1   | Step SR Task SR |              |                 | EA AF 1         |                 | Step SR Task SR |                | EA AF 1        |                | Step SR Task SR |\n| Multi- ScribeAgent-Small | M2W                                 | 42.6                                | 50.1         |         39.7 | 0               | 44.9         | 50.1            | 41.6            | 0.6             | 44.1            |                | 51.4           | 41.4           | 0               |\n| Multi- ScribeAgent-Small | M2W+Subchild                        | 42.6                                | 50.1         |         39.8 | 0               | 45.2         | 50.1            | 41.5            | 0.6             | 44.3            |                | 51.4           | 41.6           | 0               |\n| Stage                    | M2W                                 | 53.5                                | 52.9         |         51.2 | 0               | 53.4         | 52.8            | 51.3            | 2.3             |                 | 53.3           | 54.7           | 51.2           | 0               |\n| Stage                    | M2W+Subchild                        | 53.8                                | 52.9         |         51.3 | 0               | 54.0         | 52.8            | 51.9            | 2.3             | 53.5            |                | 54.7           | 51.4           | 0               |\n| Direct                   | M2W                                 | 28.6                                | 50.1         |         26.8 | 0               | 27.6         | 50.1            | 25.6            | 0               | 32.0            |                | 51.4           | 29.9           | 0               |\n| Direct                   | M2W+Subchild + Attr Match 48.8 60.8 |                                     |              |         48.3 | 5.5             | 58.0         | 66.2            | 56.7            | 6.8             | 52.9            |                | 62.1           | 52.4           | 6.5             |\n| Gen                      | M2W                                 | 38.0                                | 52.9         |         35.6 | 0               | 34.1         | 52.7            | 32.5            | 0               | 39.4            |                | 54.7           | 37.3           | 0               |\n| Direct                   | M2W                                 | M2W+Subchild + Attr Match 58.0 63.8 |              |         52   | 5.7             |              | 67.3 69.4       |                 | 59.8            | 11.8            | 62.0           | 63.7           | 52.9           | 10.8            |\n\n## A.4.2 REFINED EVALUATION\n\nAs mentioned in the main text, we improve the Mind2Web evaluation from two perspectives:\n\n- · Subchild label relaxation: We hypothesize that the distribution gap between our training data for ScribeAgent and the Mind2Web test set could be due to Mind2Web preferring ancestor/parent nodes in the HTML tree, while ScribeAgent's training data prefers lower HTML elements. To this effect, we relax the Mind2Web set of positive candidates to include not only the positive candidates, but also their children (direct children and grandchildren).\n- · Attribute matching: Direct generation setting enables higher degree of freedom in element selection. To address scenarios where the predicted element has the same function as the ground truth but is in a different location, we enhance the direct generation evaluation by introducing an element attribute comparison step. Rather than merely comparing the node ID of the predicted and the ground truth elements, we also evaluate the tag and text attributes (e.g., the text displayed on a button). If these attributes match, we consider the prediction to be correct as it has identical functionality.\n\nLastly, we note that in Mind2Web, whenever there is a textarea or an input tag, the expected behavior is to directly execute the type action. However, our model is trained to first click on the input element and then perform the type action. Thus, for actions predicted on textarea or input tags, we adjust our model to replace click actions with type actions and then compare with the ground truths.\n\nTable 11 presents the improved performance of ScribeAgent after refining the evaluation method, showing significant gains in both settings. We find that the label relaxation strategy helps bridge part of the distribution gap, and our multi-stage pipeline effectively covers most of the gains from this label relaxation strategy by using the Mind2Web ranker. However, inspecting cases that are not covered by label relaxation, we found that there still remains a distribution gap. As a result, there is large room for improving the evaluation criteria of text-based benchmark to bridge this gap.\n\n## A.5 WEBARENA EXPERIMENT DETAILS\n\n## A.5.1 FOUR-STAGE PIPELINE\n\nFor the most up-to-date prompts, please refer to our GitHub.\n\nStage 1: GPT-4o refines the intent. We use the following prompt:\n\nI have a simple task objective related to {domain}, rewrite it into a single paragraph of detailed step-by-step actions to achieve the task. When revising the objective, follow the rules:\\\\ - Assume you are already on the correct starting website and are logged in.\\\\ - Do not include any newlines, tabs, step numbers in the rewritten objective.\\\\ - Follow the example as much as possible.\\\\ {In-context demonstrations for domain rules}\\\\ Here is an example:\\\\ Simple Task Objective: {in-context demonstration}\\\\ Detailed Task Objective: {in-context demonstrations}\\\\\n\nNow, rewrite the following objective:\n\nStage 2: We process the environment-generated DOM using our preprocessing procedure. When the DOM length exceeds the 32K context window, we chunk the DOM sequentially and run the prediction workflow on each piece. For each piece of DOM, we call ScribeAgent multiple times to obtain multiple valid actions. We use the following generation configuration: do sample=True, top p=0.95, temperature=0.6. We then aggregate all possible actions, pick the top candidates, and prompt GPT-4o to select the best candidate using the following prompt:\n\nYou are an autonomous agent helping users to solve web-based tasks. These tasks will be accomplished through series of actions. The information you'll have includes:\\\\ - The user's objective\\\\ - The current web page's URL\\\\ - The current web page's accessibility tree\\\\ - Previous steps performed by the user, where each step includes a description of the action and the target web element\\\\ - Several proposed next steps, labeled by ''No.\"\\\\ Your goal is to select the best next step that can complete the task and output this candidate's number, follow the following rules:\\\\ - Do not repeat previous steps\\\\ - Reject candidates with incorrect intentions, e.g., searching for an item different from the one specified in the objective\\\\ - Reject candidates with factual errors, e.g., the description and the chosen web target do not match\\\\ - Only output a single number after to represent the selected candidate but not explanation\\\\ Now analyze the following case:\n\n## Stage 3: GPT-4o maps the agent output to accessibility tree format using the following prompt:\n\nYou are an autonomous agent helping users to solve web-based tasks. These tasks will be accomplished through series of actions. The information you'll have includes:\\\\ - The user's objective\\\\ - The current web page's URL\\\\ - A snippit of the current web page's HTML\\\\ - A snippit of the current web page's accessibility tree\\\\ - Previous steps performed by the user\\\\ Your goal is to translate a proposed next step, which consists of an action and a HTML element, into the following format:\\\\ - 'click [accessibility tree id]': This action clicks on an interactive (non-static) element with a specific id. Note this id is the number inside ''[]\" in the accessibility tree, not the HTML attribute ''node\". Brackets are required in the response. For example, a valid response is ''click [1234]\"\\\\ - 'type [accessibility tree id] [content]': Use this to type the content into the field with a specific id in the accessibility tree. For example, a valid response is ''type [1234] [New York]\". The second bracket should include everything that needs to appear in the textbox, but not only the added content. Do not change the letter case\\\\ - 'press [key\\_comb]': Simulates pressing a key combination on the keyboard (e.g., press [PageDown], press [Enter])\\\\ - 'go\\_back': Return this when the current web page does not contain useful information and the user should go back to the previous web page\\\\ When mapping the next step into actions in the above formats, follow the following rules:\\\\ - Take the user's objective into consideration, so the action must help complete the task\\\\ - Do not repeat previous steps\\\\\n\n- Only output a single step in the above format but not explanation\\\\ Note also: {in-context demonstration of rules}\\\\ Now analyze the following case:\n\nThe action is then returned to the environment for execution.\n\nStage 4: GPT-4o evaluates if the task objective is achieved. For operational tasks, if the task is completed, nothing is returned. For information seeking tasks, if the task is completed, GPT-4o retrieves the answer to the question. The prompt looks like the following:\n\nYou are an autonomous agent helping users to solve web-based tasks. These tasks will be accomplished through series of actions. The information you'll have includes:\\\\ - The user's task, including a high-level objective and a more detailed illustration\\\\\n\n- The current web page's URL and accessibility tree\\\\ - Previous steps performed by the user, where each step includes a description of the action and the target web element\\\\ You should follow the rules: {in-context demonstration rules}\\\\ You will decide whether the task specified by the high-level objective is completed (which means the ** last ** step of the detailed instruction is completed and the current webpage completes the task) and respond ''completed\" or ''incomplete\". If the task requires returning a number or a string and the answer can be obtained in the current webpage, reply ''completed, [answer]\" where ''[answer]\" is the number or string. If the task requires finding a webpage and the current webpage satisfies the requirement, reply ''completed, [answer]\" where ''[answer]\" is the current URL. Now analyze the following case. First provide the reasonings. Then summarize the answer with ''Summary:\", followed by ''completed\" or ''incomplete\", followed by the answer to the question if applicable. Do not include newlines after ''Summary:\".\n\n## A.5.2 SCROLLING ACTIONS AND COMBOBOX SELECTION\n\nIn our data collection process, we capture the full DOM from a system perspective, which inherently includes the entire webpage as observed from the backend. This method differs from user-centric data collection, where only the elements within the visible browser viewport are captured. Consequently, there is no concept of scrolling in our training datasets since all elements are already fully accessible in the captured data.\n\nHowever, we recognize the importance of scroll actions in solving WebArena from a user perspective. To address this, before issuing any action to the environment, our multi-agent system includes a viewport check that uses the bounding box position to determine if the target element is within the visible webpage area. If not, the system manually inserts necessary scroll actions to bring the element into view. This ensures accurate interaction with web elements in a typical user scenario.\n\nTo handle combox selection, our agent discovers a workaround that bypasses the need for scrolling through comboboxes. Specifically, after clicking on the combobox, it types the name of the desired item in the combobox, which brings the item to the top of the dropdown menu. Then, the agent can simply click the item or press Enter. This approach avoids the need for scrolling and is especially effective in densely populated lists. It improves the task success rate on a large number of Map, Reddit, and GitLab tasks.\n\n## A.5.3 GPT-4O-ONLY SETTING\n\n## When we use GPT-4o for stage 2, we use the following prompt:\n\nYou are an autonomous intelligent agent tasked with solving web-based tasks. These tasks will be accomplished through the use of specific actions you can issue. Here's the information you'll have:\\\\\n\n- - The user's objective: This is the task you're trying to complete.\\\\\n- - The current web page's URL: This is the page you're currently navigating.\\\\\n- - The current web page's HTML: Each element is assigned with an unique ID,\n\ndenoted by the attribute ''node\".\\\\\n\nThe actions you can perform include:\\\\\n\n- - mouse\\_click\\_action: click\\\\\n- - keyboard\\_sequence\\_action: type a sequence of characters\\\\\n- - keyboard\\_combination\\_action: press a set of keys together\n\n(e.g., hotkey like ctrl+c)\\\\\n\nYou will generate a step-by-step guide to complete the task based on the given information. At each step, you can perform only one action to one web element. The output should be in the correct format: a single step consisting of a text description, an action, as well as the node and HTML of the target web element to perform the action. Be coherent, concise, and accurate in your response. Do NOT use any special characters (e.g., '' * \", ''#\", etc.) in your response. Follow EXACTLY the format of the response below.\\\\\n\nHere is one example:\\\\ Description: click ''Users\"\\\\ Action: mouse\\_click\\_action\\\\ Node: 93\\\\\n\nTarget: <a class=''slds-tree\\_\\_item-label\" node=''93\">\\\\\n\nNow complete the following task by generating a single next step:\n\n## A.5.4 RESULTS\n\nBelow, we provide example action trajectories produced by our agent and explanations to shed light on its decision making process.\n\nDomain: Shopping\n\nTask ID: 466\n\nIntent: Add 2 Hawaiian Bamboo Orchid Roots #zc50 - by Discount Hawaiian Gifts to my wish list. Actions:\n\n- 1. click [1585] where [1585] is combobox 'Search' autocomplete: both hasPopup: listbox required: False expanded: False → Click on the search bar.\n- 2. type [1585] [Hawaiian Bamboo Orchid Roots #zc50 - by Discount Hawaiian Gifts ] where [1585] is combobox 'Search' focused: True autocomplete: both hasPopup: listbox required: False expanded: True → Type the target item name.\n- 3. press [enter] → Do the search.\n- 4. scroll [down] → Scroll down the page to find out the target item.\n- 5. click [4859] where [4859] is link '2 Hawaiian Bamboo Orchid Roots #zc50 - by Discount Hawaiian Gifts' → Open the item page.\n- 6. click [6733] where [6733] is link 'Add to Wish List' → Add item to wish list.\n\n## Domain: CMS\n\nTask ID: 244\n\nIntent: Show me the email address of the customer who is the most unhappy with Olivia zip jacket.\n\n## Actions:\n\n- 1. click [108] where [108] is link 'MARKETING' → Click on the 'MARKETING' button in the side bar.\n- 2. click [419] where [419] is link 'All Reviews → Clicking the side bar shows more option. We can then click on the 'All Reviews' button.\n- 3. click [2195] where [2195] is button 'Reset Filter' → Remove all previous set filters.\n- 4. type [4435] [olivia] where [4435] is textbox '' required: False → In the search box, type 'olivia'.\n- 5. press [enter] → Do the search\n- 6. stop [emma.lopez@gmail.com] → Inspect all listed reviews and return the email of the negative review.\n\nDomain: Reddit\n\nTask ID: 403\n\nIntent: Change my reddit bio to 'Awesome Prompt Artist'\n\nActions:\n\n- 1. click [12] where [12] is button 'MarvelsGrantMan136' hasPopup: menu expanded: False → Click my user name on the top right corner.\n- 2. click [249] where [249] is link 'Profile' → After the drop-down menu shows up, click 'Profile'.\n- 3. click [886] where [886] is link 'Edit biography' → Click the 'Edit biography' button.\n- 4. type [2456] [Awesome Prompt Artist] where [2456] is textbox 'Biography' required: False describedby: user biography biography help → Type the target content.\n- 5. click [2474] where [2474] is button 'Save' → Save the new profile.\n\nDomain: GitLab\n\nTask ID: 293\n\nIntent: Show me the command to clone Super Awesome Robot with SSH.\n\nActions:\n\n- 1. click [1507] where [1507] is textbox 'Search GitLab' required: False → Click on the search bar.\n- 2. type [1516] [Super Awesome Robot] where [1516] is searchbox 'Search GitLab' focused: True describedby: search-input-description → Type the repo name in the search bar.\n- 3. click [2082] where [2082] is link 'Convex Eggtart / Super Awesome Robot' → Click on the correct repo.\n- 4. click [2699] where [2699] is link 'Clone' → Click on the 'Clone' button.\n- 5. stop [git clone ssh://git@metis.lti.cs.cmu:2222/convexegg/super awesome robot.git] → Read the command from the pop-up window.\n\nDomain: Maps\n\nDomain: Maps\n\nTask ID: 7\n\nIntent: Tell me the full address of all international airports that are within a driving distance of 50 km to Carnegie Mellon University.\n\nActions:\n\n- 1. click [35] where [35] is textbox 'Search' focused: True required: False → Click on the search box.\n- 2. type [35] [airport Pittsburgh] where [35] is textbox 'Search' focused: True required: False → Type 'airport Pittsburgh' in the search box.\n- 3. stop [Pittsburgh International Airport, Airport Boulevard, Findlay Township, Allegheny County, 15231, United States.] → Return 'Pittsburgh International Airport, Airport Boulevard, Findlay Township, Allegheny County, 15231, United States.' as the answer.\n\n**References**:\nhttps://arxiv.org/pdf/2411.15004\n\n**Output**:\n\n<blog>blog content here in markdown format </blog>\n \n<tags>[tag1,tag2]</tags>",
          "additional_kwargs": {},
          "response_metadata": {},
          "type": "human",
          "name": null,
          "id": null,
          "example": false
        },
        {
          "content": "You are an expert copy writer who can write blog posts for Linkedin in Academic style of writing.\n\nYou will be given a short tweet text and reference article, your task is to write a blog post in correct markdown format and add references.\n\nThe type of blog would be of NonAI, Technical categories. Make sure to write the blog inside <blog> tags. Also determine proper hash tags and extract inside <tags> in list form\n\nAdd references section in the end, having reference links mentioed\n\n**Tweet**:\nResearch paper by @ScribeHow and @mldcmu\\n\\nhttps://t.co/s0cH0owC94 https://t.co/i6scxS4MeM\n\n**Article**:\n## SCRIBEAGENT: TOWARDS SPECIALIZED WEB AGENTS USING PRODUCTION-SCALE WORKFLOW DATA\n\nJunhong Shen ∗\n\nAtishay Jain ∗\n\nZedian Xiao ∗\n\nIshan Amlekar\n\nMouad Hadji\n\nAaron Podolny\n\nAmeet Talwalkar\n\n{ junhongs, atalwalkar } @cs.cmu.edu { atishay, mark, ishan, mouad, aaron } @scribehow.com\n\nAI/ML, Scribe Machine Learning Department, CMU\n\n## ABSTRACT\n\nLarge Language Model (LLM) agents are rapidly improving to handle increasingly complex web-based tasks. Most of these agents rely on general-purpose, proprietary models like GPT-4 and focus on designing better prompts to improve their planning abilities. However, general-purpose LLMs are not specifically trained to understand specialized web contexts such as HTML, and they often struggle with long-horizon planning. We explore an alternative approach that fine-tunes open-source LLMs using production-scale workflow data collected from over 250 domains corresponding to 6 billion tokens. This simple yet effective approach shows substantial gains over prompting-based agents on existing benchmarks-ScribeAgent achieves state-of-the-art direct generation performance on Mind2Web and improves the task success rate by 7.3% over the previous best text-only web agents on WebArena. We further perform detailed ablation studies on various fine-tuning design choices and provide insights into LLM selection, training recipes, context window optimization, and effect of dataset sizes.\n\n## 1 INTRODUCTION\n\nLarge language model (LLM) agents have advanced significantly in web navigation. They can carry out user-specified tasks in multiple steps by reasoning on their own what actions to take and what external resources to interface with. Recent studies (Zheng et al., 2024; Lai et al., 2024; Zhang et al., 2024; Song et al., 2024) have shown that, with better planning and exploration strategies, LLM agents can independently solve various web tasks ranging from simple navigation, such as locating a specific Wikipedia page, to more complex operations, such as booking flights or restaurants.\n\nDespite these improvements, the performance of existing web agents on research benchmarks remains significantly below human levels (Deng et al., 2023; Zhou et al., 2024; Drouin et al., 2024). One possible reason is their dependence on general-purpose LLMs. Indeed, all top-performing agents like WebPilot (Zhang et al., 2024), AWM (Wang et al., 2024b), and SteP (Sodhi et al., 2024) rely on prompting proprietary models like GPT-4 (OpenAI, 2024a). These general-purpose LLMs are not optimized for interpreting web contexts such as HTML or accessibility trees; their pretraining and alignment processes do not address navigation-related challenges; and their proprietary nature presents a major obstacle in adapting them to web environments via continual training.\n\nFigure 1: Left: Most existing LLM web agents are built on top of general-purpose, proprietary models like GPT-4 and rely heavily on prompt engineering. Their performance is enhanced by leveraging external planning, reasoning, and memory modules. Right: We explore an alternative way to develop specialized agents by fine-tuning open-source LLMs using a large set of high-quality, realworld workflow data. This significantly boosts agent's navigation and planning capacity, enabling it to outperform proprietary models with a smaller LLM backbone, thereby reducing serving costs.\n\n<!-- image -->\n\nIn this work, we explore an alternative approach by fine-tuning open-source LLMs with a large set of real-world workflow data 1 to develop specialized web agents (Figure 1). Through extensive experiments, we show that this approach not only boosts the web understanding and planning abilities of LLMs, achieving state-of-the-art results on various benchmarks, but also allows us to develop agent models significantly smaller than proprietary LLMs, reducing the serving costs. Our empirical results suggest that large-scale, high-quality, real-world data can be essential to agent development.\n\nSpecifically, we collect a set of proprietary workflow data representing action sequences executed by real users in real web environments through Scribe. This dataset encompasses a large and diverse spectrum of websites (over 250 domains and 10,000 subdomains), task objectives, task difficulty, and task length. Each step in the workflow features not only the raw HTML-DOM of the website but also a comprehensive documentation of the action, including action description in natural language, mouse or keyboard operation, and the CSS selector of the target HTML element. We reformat the data into a next-step prediction formulation and fine-tune a set of open-source LLMs via the parameter-efficient LoRA (Hu et al., 2022). After preprocessing and reformatting, our training dataset contains more than 6 billion tokens.\n\nWith access to this production-scale dataset, we develop ScribeAgent, the first family of specialized, single-stage LLM agents capable of directly generating the next step based on the website's DOM and action history. This is in contrast with previous fine-tuned agents that require multiple stages to produce an action, e.g., first narrowing down to a set of target element candidates and then selecting one from the candidates (Deng et al., 2023). To evaluate the capacity and generalization ability of ScribeAgent, we test it on public benchmarks without any further training. ScribeAgent significantly outperforms existing GPT-4-based and multi-stage agents. Notably, the 32B-parameter ScribeAgent-Large achieves state-of-the-art direct generation performance on Mind2Web (Deng et al., 2023), with step success rate surpassing the baselines by 5-10% across all test sets. On the end-to-end task execution benchmark WebArena (Zhou et al., 2024), our 7B ScribeAgent-Small improves the previous best task success rate from 45.7% to 51.3%. ScribeAgent-Large achieves an even better task success rate of 53%, marking the highest performance among text-only LLM agents.\n\nBeyond the empirical results, our work also provides several insights valuable for future web agent research: (1) we show that direct fine-tuning on highly structured inputs (HTML-DOM) is feasible and can improve the agent's ability in identifying the correct target; (2) we identify an effective HTML preprocessing strategy that balances between preserving essential information and minimizing context length; (3) we provide a thorough analysis on various design choices in fine-tuning, such as LLM backbone and context window selection; (4) we illustrate how fine-tuning improves agent performance as dataset size increases.\n\nOur work highlights the potential of building web agents via specialized fine-tuning with productionscale data. This approach not only improves agents' capabilities relative to prompt-engineered alternatives, but also reduces inference costs due to the smaller sizes of open-source LLMs. While our work focuses on studying the effect of fine-tuning, ScribeAgent can be extended to leverage more sophisticated search or memory modules (Koh et al., 2024; Wang et al., 2024b), combined with existing planning frameworks (Yao et al., 2022; Madaan et al., 2023; Shinn et al., 2023), or integrated into multi-modal web agent systems as the text model (Wang et al., 2024a). We view ScribeAgent as an important step towards developing AI assistants and fully automated agents for real-world web applications.\n\n## 2 RELATED WORK\n\nPrompting-based agent frameworks. The majority of web agent works reuse existing LLMs and propose different prompting strategies to improve action prediction. One line of research focuses on exploiting previous experience via self-feedback (Sun et al., 2023) or in-context demonstrations (Fu et al., 2024; Zheng et al., 2024; Wang et al., 2024b; Ou et al., 2024; Shen et al., 2024b). A separate line of work centers around encouraging exploration by including external evaluators (Pan et al., 2024), using synthesized instructions (Murty et al., 2024b), or applying more sophisticated search algorithms like stack (Sodhi et al., 2024), best-first tree search (Koh et al., 2024), or Monte Carlo Tree Search (Zhang et al., 2024). Despite the research efforts, these prompting methods rely heavily on the quality of the LLM used. Open-source models such as LLaMA (Dubey et al., 2024), Code LLaMA (Rozi'ere et al., 2024), and Flan-T5 (Chung et al., 2022) generally underperform proprietary models like GPT-4. However, fine-tuning proprietary LLMs can often be costly and challenging, as it is restricted to being done through APIs. This implies an opportunity for enhancing open-source LLMs to match or outperform proprietary agents.\n\nFine-tuning-based web agents. Compared to developing better reasoning and planning frameworks, comparatively less attention has been given to optimizing the LLMs themselves to better handle web environments. Due to the difficulty of directly generating a single target element from the raw HTML, which often contains thousands of elements, existing work mostly focuses on multistage prediction. MindAct (Deng et al., 2023) proposes a two-stage pipeline that first uses a small LMto filter the web elements and then uses a more powerful LM to select from the filtered elements in a multi-choice question answering format. Both LMs can be fine-tuned using the Mind2Web dataset. WebAgent (Gur et al., 2023) uses HTML-5 to first process the HTML and then fine-tunes a 540B Flan-UPalm to generate code for controlling web pages. More recently, AutoWebGLM (Lai et al., 2024) trains a single ChatGLM3 6B (GLM et al., 2024) using a combination of curriculum learning, reinforcement learning, and rejection sampling fine-tuning. NNetnav (Murty et al., 2024a) leverages synthetic demonstrations collected by zero-shot LLM exploration and hindsight relabeling to fine-tune open-source models into web agents. Despite the complicated training and inference procedures, these methods often underperform agents that prompt GPT-4. In contrast, our work shows that given sufficient high-quality workflow data, fine-tuning a single LLM can achieve strong performance. We note that the newly released OpenAI o1 (OpenAI, 2024c) can be viewed as a specialized agent with a complicated planning framework. Nonetheless, we show in Section 4.1 that ScribeAgent outperforms o1-preview by a large margin on our proprietary dataset. Moreover, while none of the training details for o1 have been released, our work provides valuable insights into data preprocessing and fine-tuning.\n\nBeyond the aforementioned work, there is an earlier line of research that fine-tunes LLMs for HTML inputs (Gur et al., 2022; Nakano et al., 2022; Liu et al., 2023). However, their primary application is question-answering tasks, such as answering 'could sunflowers really track the sun across the sky', and they cannot be used to generate a sequence of actions based solely on the user objective.\n\nLastly, we note that an emerging line of research has committed to developing multi-modal web agents that use screenshots along with HTML observations. Examples include CogAgent (Hong et al., 2023), SeeClick (Cheng et al., 2024), WebGUM (Furuta et al., 2024), WebVoyager (He et al., 2024), and AWA 1.5 (JaceAI, 2024). However, our current version of ScribeAgent focuses exclusively on text-based inputs due to the lack of effective visual preprocessing schemes. Thus, we do not compare with the aforementioned multi-modal methods in our experiments and leave developing multi-modal ScribeAgent as future work.\n\n## 3 METHOD\n\nIn this section, we first overview the general setup of solving web-based tasks with LLM agents. Then, we detail our proposed method to develop specialized agents from open-source LLMs.\n\n## 3.1 GENERAL SETUP\n\nWe consider solving a web-based task as a sequential decision-making process guided by a highlevel objective. For each task, the user first specifies an objective and a starting web page. Then, at every step, the agent outputs an action based on the task objective, the current web page, and the history. Formally, denote the user objective as q . The web environment is governed by a transition function T that can evolve over time. The agent is instantiated by a language model L . At each time step t , the agent observes o t produced by the environment state s t and observes the history h t = H ( o 1: t -1 , a 1: t -1 ) . It outputs an action a t = L ( q, o t , h t ) , which is executed in the environment, and the state changes correspondingly s t +1 = T ( s t , a t ) . This iterative process stops when the agent issues a stop signal, or a task termination condition is met, such as we have reached a predefined maximum number of steps.\n\nFor single-modal, text-only agents, the observation o t typically consists of the website's URL, the HTML-DOM (Object Model for HTML, which defines HTML elements and their properties, methods, and events), and potentially the accessibility tree (a representation that can be understood by assistive technologies like screen readers). Since the raw HTML-DOM is often long and contains redundant structural information, most methods employ preprocessing and pruning strategies, which could be as simple as retaining a fixed set of HTML tags and attributes or more complex ones like LLM-based element ranking and filtering (Deng et al., 2023).\n\nThe action a t emulates the keyboard and mouse operations available on web pages. The most general action space in existing work consists of element operations, such as clicking, typing, and key combination pressing; tab actions, such as opening, closing, and switching between tabs; navigation actions, such as going forward and backward in the browsing history (Zhou et al., 2024).\n\nAs discussed earlier, previous web agent work focuses on presenting useful demonstrations through h t or iteratively revising a t to improve the quality of the predicted next step. In contrast, we explore whether we can improve the model L itself by learning from a vast amount of data and incorporating more information into o t , such as the natural language description and HTML representation of a action. We detail our approach in the next section.\n\n## 3.2 SCRIBEAGENT: SPECIALIZING WEB AGENTS THROUGH FINE-TUNING\n\n## 3.2.1 COLLECTING PRODUCTION-SCALE DATA\n\nWe collected a large set of real-world, user-annotated, proprietary data through Scribe, a software that streamlines the creation of step-by-step guides for web-based tasks. Scribe allows users to record their interactions with the web through a browser extension and converts the interactions into well-annotated instructions, which can be then customized to specific business needs. The collected dataset consists of everyday workflows in common web application domains, encompassing customer relationship management (CRM) tools like HubSpot and Salesforce; productivity tools like Notion and Calendley; social platforms like Facebook and LinkedIn; shopping sites like Amazon and Shopify; and many others.\n\nEach workflow features a high-level user objective and a step-by-step documentation of the action sequence to achieve the task. The objective spans a wide range of topics, such as 'add a user in a Salesforce' or 'invite someone to manage Facebook ad accounts'. Each step contains the following information: the current web page's URL, raw HTML-DOM, a natural language description of the action performed, the type of action, and the autogenerated CSS selector to identify the action target. There are three types of actions in the dataset:\n\n- · mouse click action : click at an element\n- · keyboard sequence action : type a sequence of characters to an element\n- · keyboard combination action : press a set of keys together (e.g., hotkey like ctrl+c)\n\nNote that there is no scroll actions in our action space since all elements are already fully accessible in the captured data. This is because we capture the full DOM from a system perspective, which inherently includes the entire webpage as observed from the backend. This method differs from usercentric data collection, where only the elements within the visible browser viewport are captured.\n\nTo ensure the quality of the data, we remove workflows with invalid selectors, i.e., the selector cannot be used to locate a target element in the DOM. We also remove non-English workflows to reduce dataset complexity and enable us to explore English-only LLMs like Mistral 7B (MistralAI, 2023). The resulting dataset is at production scale: using raw data collected over a two-month period, we are able to extract workflow data from more than 250 domains and 10,000 subdomains with an average task length of 11 steps, which correspond to about 6 billion training tokens. This large-scale, high-quality, real-world dataset is unmatched in prior web agent research.\n\nSince this dataset is collected from real users and might contain sensitive and confidential information, it will not be released to the public to protect user privacy. The dataset is solely for research purposes and has been anonymized to prevent the identification of any individual.\n\n## 3.2.2 PREPROCESSING\n\nFor ScribeAgent, we consider an observation space consisting mainly of the URL and HTML-DOM. Specifically, HTML-DOM provides agents with all structural and content information about the web page that are essential for generating the next step and long-term planning. For instance, while a drop-down menu may not be visible on the website before expansion, the agent can detect the menu items from the DOM and determine whether to click and expand it. We do not use accessibility tree to develop ScribeAgent because it may lose information about the HTML elements, such as the drop-down items, and does not generalize across different browsers and devices.\n\nGiven our observation space, a subsequent problem is that the DOM can be quite long and exceed the context window of prevailing open-source LLMs. To reduce the DOM sizes, we propose a pruning algorithm that maintains the essential structure and content while eliminating redundant or disruptive elements that could hinder the LLM's understanding. Specifically, we first use the BeautifulSoup library (Richardson, 2007) to remove non-essential components such as metadata, CSS, and JavaScript. Then, we utilize a tag-attribute white list to retain useful tag level information like retaining interactive elements. Since some attribute values can contain random character sequences that do not provide useful information, we propose a novel detection method that removes the attributes with character-to-token-ratio smaller than 2, i.e., len ( s ) len ( tokenizer ( s )) < 2 , where s denotes the value string. Intuitively, if each character in a string is encoded using a separate token, it is highly likely that the string is not semantically meaningful. Lastly, we remove the comments and extra whitespaces to clean up the DOM. After pruning, we assign each tag in the HTML with a unique ID by traversing the HTML tree from bottom to top. More details about preprocessing and analysis on the tokenizer-pruning method can be found in Appendix A.1.\n\nWerestrict the action space of ScribeAgent to the three types of operations specified in Section 3.2.1. To preprocess the action sequences, we rewrite each step into five lines as follows:\n\n1. Description: Click the 'Menu' button to browse all food options\n\nAction: mouse click action\n\nNode: 832\n\nTarget: < svg class='open-hamburger-icon' node='832' role='img' >\n\nThe first line represents the current time step. The second line is the natural language description of the action, which can help LLMs to learn about the rationale behind applying a specific action. The third line is one of the three operations in the action space. The fourth line is the unique ID assigned to the target element. The last line details the HTML tag and attributes, which can be directly obtained from the processed DOM.\n\nFor the history, we consider only previous actions, omitting previous observations due to the extensive length of the DOMs. That is, h t = a 1: t -1 . Therefore, at each step, ScribeAgent will be given the task objective, URL, HTML-DOM, and all previous actions in the aforementioned five-\n\nTable 1: Performance of different LLMs fine-tuned on 1B workflow tokens on the test split of our proprietary dataset. We highlight the best results for small/medium/large models. EM is short for Exact Match.\n\n| Model                      | # Params   | Before Fine-Tuning   | Before Fine-Tuning   | After Fine-Tuning   | After Fine-Tuning   |\n|----------------------------|------------|----------------------|----------------------|---------------------|---------------------|\n|                            |            | EM(%)                | Calibrated EM (%)    | EM(%)               | Calibrated EM (%)   |\n| Mistral-7B-Instruct-v0.3   | 7B         | 3.89                 | 5.13                 | 19.92               | 26.31               |\n| Qwen2-7B-Instruct          | 7B         | 6.06                 | 7.92                 | 29.34               | 38.72               |\n| Llama-3.1-Instruct-8B      | 8B         | 1.42                 | 1.88                 | 28.34               | 37.42               |\n| Qwen2.5-14B-Instruct       | 14B        | 8.79                 | 11.60                | 31.76               | 41.89               |\n| Codestral-22B-v0.1         | 22B        | 4.53                 | 6.08                 | 31.11               | 41.25               |\n| Qwen2.5-32B-Instruct       | 32B        | 10.02                | 13.21                | 32.98               | 43.51               |\n| Mixtral-8x7B-Instruct-v0.1 | 56B-A12B   | 7.35                 | 9.82                 | 28.38               | 37.49               |\n| Qwen2-57B-A14-Instruct     | 57B-A14B   | 5.72                 | 7.51                 | 31.02               | 40.10               |\n\nline format. Its goal is to output the next action a t = L ( q, o t , a 1: t -1 ) that helps complete the task. In Appendix A.2, we provide an example of a full workflow.\n\nLastly, during our inspection, we find that 10% of the action descriptions in the dataset are not informative (e.g., 'click here'). In these cases, we use GPT-4o (OpenAI, 2024b) to regenerate the action description from screenshots. We provide the prompt as well as examples of the regenerated action descriptions in Appendix A.3.1.\n\n## 3.2.3 FINE-TUNING WITH LORA\n\nAfter preprocessing, we divide the dataset into two splits. The test set comprises of 1200 workflows with diverse objectives and domains. We use the remaining workflows as the training data to adapt LLMs via standard supervised fine-tuning. Note that for each fine-tuning example, the label is a single next-step instead of all remaining steps needed to complete the task. The agent is trained to generate all information in the five-line format described above, including the natural language description.\n\nTo reduce fine-tuning cost, we opt for the parameter efficient method LoRA (Hu et al., 2022) instead of full fine-tuning, since we have not observed significant performance gain by updating more parameters. We also follow previous work (Zhao et al., 2023; Shen et al., 2023) to fine-tune the layernorms in addition to the LoRA adapters. Based on empirical observations, we set the fine-tuning epoch to 2, effective batch size to 32, LoRA rank to 64 and α to 128. We use a cosine scheduler with 30 warmup steps and a learning rate of 1e-4.\n\n## 3.2.4 EXPLORING THE DESIGN SPACE\n\nThere are multiple design choices for ScribeAgent that might affect the prediction accuracy, finetuning cost, and inference latency. We focus on three aspects and perform detailed ablation studies to find out the optimal modeling and training configurations.\n\nPretrained LLM Selection. Intuitively, the quality of a fine-tuned web agent should be relevant to the quality of the pretained LLM. We identify two axes that are crucial to performance-model architecture and model size-and explore seven open-source LLMs spanning these axes: Llama 3.1 8B (Dubey et al., 2024), Mistral 7B (MistralAI, 2023), Mixtral 8x7B (MistralAI, 2024b), Qwen2 7B (Yang et al., 2024a), Qwen2 57B (Yang et al., 2024a), Qwen2.5 14B (Team, 2024), Qwen2.5 32B (Team, 2024), and Codestral 22B (MistralAI, 2024a). We fine-tune these models with 1 billion training tokens and evaluate their performance on the test split of the dataset we collected.\n\nGiven that many of the evaluated LLMs have a maximum context window of approximately 32K, and the processed DOM can exceed this limit, we divide the DOM sequentially into chunks that fit into the context window. For fine-tuning, we use the chunk containing the correct target, but for evaluation, we use the last chunk since the target's location is not known beforehand. When evaluating at a 32K context window, 25% of the test data do not have the correct target tag in the DOM, i.e., these tasks are unachievable. Thus, we compute two metrics for evaluation: (1)\n\nexact match (EM) measures the model's ability to select exactly the same HTML tag as the ground truth; (2) calibrated exact match (Calibrated EM, or CEM) measures the percentage of correct target predictions where the target tag was present in the truncated HTML DOM, i.e., it is EM on the set of examples where the observation contains sufficient information to complete the task. As we scale the context window, these two metrics converge.\n\nWe report the performance of different LLMs before and after fine-tuning in Table 1. Notably, for all models, specialized fine-tuning drastically increases the prediction accuracy. Both before and after fine-tuning, the Qwen family demonstrates better EM and CEM across small, medium, and large models. We observe performance gains as model size increases. For example, the calibrated EM for Qwen2 57B is higher than its 7B counterpart. Qwen2.5 32B is also better than Qwen2.5 14B. Mixtral 8x7B outperforms Mistral 7B by a large margin as well. However, fine-tuning larger models is significantly more resource-intensive-while Qwen2 7B can be fine-tuned using 8 H100 GPUs in just one day, Qwen2 57B takes over a week using the same hardware configuration. Larger models also incur longer inference times and require multiple GPUs even at a 32K context length. To balance effectiveness and efficiency, we develop two versions of ScribeAgent using Qwen2 7B and Qwen2.5 32B, respectively. The Qwen2 7B model offers an optimal balance between prediction accuracy and the costs associated with fine-tuning and inference. Meanwhile, the Qwen2.5 32B model provides stronger performance when we have sufficient computational resources.\n\nContext Window Length. We evaluate the models with 65K context window to add additional context and increase the rate of solvable tasks (Table 2). On both Qwen2 and Qwen2.5, scaling the context window from 32K to 65K leads to approximately 2% performance boost for Exact Match but approximately 2 . 5% performance drop for Calibrated Exact Match. We hypothesize that this performance degradation might be due to rotary position embedding (Su et al., 2021) and the fact that it becomes\n\nTable 2: Ablations on context window length.\n\n| Model       | Context   |   EM(%) |   CEM(%) |\n|-------------|-----------|---------|----------|\n| Qwen2 7B    | 32K       |   29.34 |    38.72 |\n| Qwen2 7B    | 65K       |   31.42 |    36.22 |\n| Qwen2.5 14B | 32K       |   31.76 |    41.89 |\n| Qwen2.5 14B | 65K       |   33.96 |    39.15 |\n| Qwen2.5 32B | 32K       |   32.98 |    43.51 |\n| Qwen2.5 32B | 65K       |   36.16 |    41.69 |\n\nharder to pick the correct target given twice as many options to choose from. Besides, we note that using 65K context window increases the inference time by approximately four times in practice.\n\nDataset Size. Lastly, we are interested in understanding the effect of fine-tuning dataset size on the agent's performance. To this end, we sample our training set without replacement into smaller subsets and fine-tune Qwen2 7B on them. Results are shown in Table 3. Plotting on a log-linear scale, we observe that there is a roughly 2% performance boost when we double our dataset size.\n\nTable 3: Ablations on dataset size. All settings are trained and evaluated with Qwen27B-Instruct and 32K context window.\n\n| # Train Tokens   |   EM(%) |   CEM(%) |\n|------------------|---------|----------|\n| 1B               |   29.34 |    38.72 |\n| 3B               |   32.65 |    43.06 |\n| 6B               |   34.96 |    46.42 |\n\nTo sum up, using our proprietary dataset, we study the effect of LLM backbone, context window, and dataset size on the agent performance. We find that (1) scaling parameter count generally improves prediction quality, but the latency and training time of large LLMs can be prohibitive; (2) using longer context window boosts model performance on EM but increases the inference time significantly; (3) training with more tokens is helpful. Based on these insights, we develop two versions of ScribeAgent: ScribeAgent-Small, based on Qwen2 7B, and ScribeAgent-Large, based on Qwen2.5 32B. Both versions are fine-tuned on the full 6B-token dataset at a 32K context window. While ScribeAgent-Large demonstrates better performance in both internal and external evaluation,s the 7B ScribeAgent-Small is cheaper to serve at inference time, particularly when compared to large-scale proprietary models.\n\n## 4 RESULTS\n\nWe evaluate ScribeAgent on three web datasets. We first consider the next-step prediction setting, where performance is evaluated only on a single next step. We show that ScribeAgent not only outperforms various general-purpose LLMs on our proprietary dataset but also achieves state-ofthe-art on the public benchmark Mind2Web (Deng et al., 2023). Then, we move to the end-to-end task completion benchmark WebArena (Zhou et al., 2024) and show that ScribeAgent augmented\n\nTable 4: ScribeAgent v.s. non-fine-tuned, general-purpose baselines on the full test set with truncation at 32K tokens.\n\n| Model             |   EM(%) |   CEM(%) |\n|-------------------|---------|----------|\n| Qwen2 7B          |    6.28 |     8.2  |\n| GPT-4o mini       |   12.6  |    13.26 |\n| GPT-4o            |   15.24 |    16.02 |\n| ScribeAgent-Small |   34.96 |    46.42 |\n| ScribeAgent-Large |   37.67 |    49.67 |\n\nFigure 2: Exact Match (EM) comparison between ScribeAgent-Small and OpenAI models across different types of websites.\n\n<!-- image -->\n\nwith GPT-4o achieves top performance among all existing agent systems. Note that we do not perform any task-specific adaptation for Mind2Web and WebArena, even when additional training data is available. This allows us to evaluate the generalization ability of ScribeAgent. Our focus is on ensuring that ScribeAgent remains versatile and robust across diverse settings, rather than optimizing for specific benchmarks.\n\n## 4.1 PROPRIETARY DATASET\n\nTo study whether specialized fine-tuning is indeed beneficial, we first compare the performance of ScribeAgent with general-purpose baselines on our proprietary test data. We consider the non-finetuned Qwen2 7B, GPT-4o, and GPT-4o mini. We use in-context demonstrations to prompt them to generate actions in the same five-line format as defined in Section 3.2.2. All OpenAI baselines in this work follow the prompt in Appendix A.3.2.\n\nResults on the full 1200 test workflows are shown in Table 4. First, we note that ScribeAgent significantly outperforms the proprietary GPT-4o and 4o mini. This shows the benefit of specialized fine-tuning over using general-purpose LLMs. Moreover, while the non-fine-tuned Qwen2 performs extremely poorly, fine-tuning with our dataset (ScribeAgent-Small) boosts its performance by nearly 6 × , which highlights the importance of domain-specific data.\n\nWe also plot the Exact Match metric for four types of commonly seen domains, including customer relationship management (CRM) tools, E-commerce platforms, productivity tools, and social platforms (Figure 2). While our agent's performance varies by domain, with a 6% gap between the best performing domain and the worst performing one, we observe that ScribeAgent consistently outperforms the general-purpose baselines across all of them.\n\nAs we were wrapping up this work, OpenAI released o1 (OpenAI, 2024c), a series of specialized models for solving complex tasks in science, coding, and math. Since it has better planning ability, we also include it in our baselines. However, we did not run the o1 models on the full test set due to cost and API call limitations. Instead, we subsample 500 workflows and compare with ScribeAgent. As shown in Table 5, o1-preview performs the best among all general-purpose baselines. However, ScribeAgent still outperforms it by a wide margin, highlighting the importance of fine-tuning on real-world web navigation data. It\n\nTable 5: Comparing ScribeAgent with OpenAI baselines on 500 test samples. All models are evaluated at a 128K context window.\n\n| Models            |   EM(%) |   CEM(%) |\n|-------------------|---------|----------|\n| o1-mini           |    17.4 |    18.32 |\n| o1-preview        |    22.6 |    23.79 |\n| GPT-4o mini       |    13.8 |    14.53 |\n| GPT-4o            |    16.6 |    17.96 |\n| ScribeAgent-Small |    47.6 |    50.11 |\n| ScribeAgent-Large |    50   |    52.6  |\n\nis important to note that ScribeAgent-Small only has 7B parameters, while ScribeAgent-Large has 32B parameters, and neither model requires additional scaling during inference. In contrast, most proprietary baselines are typically larger in size and require more compute at inference. This makes ScribeAgent a better choice in terms of accuracy, latency, and cost.\n\nTable 6: ScribeAgent achieves state-of-the-art zero-shot performance on Mind2Web. EA is short for element accuracy, AF 1 is short for action F 1 , and SR is short for success rate. We note that the three categories are based on increasing level of domain generalization difficulty. However, since we do not train on Mind2Web data, our performance is similar across different test sets. Numbers are bolded for each method category.\n\n| Method          |                              | Cross-Task   | Cross-Task   | Cross-Task   | Cross-Task      | Cross-Website   | Cross-Website   | Cross-Website   | Cross-Website   | Cross-Domain   | Cross-Domain   | Cross-Domain   | Cross-Domain    |\n|-----------------|------------------------------|--------------|--------------|--------------|-----------------|-----------------|-----------------|-----------------|-----------------|----------------|----------------|----------------|-----------------|\n|                 |                              |              | EA AF 1      |              | Step SR Task SR |                 | EA AF 1         |                 | Step SR Task SR |                | EA AF 1        |                | Step SR Task SR |\n|                 | Uses M2W Train Set           |              |              |              |                 |                 |                 |                 |                 |                |                |                |                 |\n|                 | MindAct (Flan-T5 B )         | 43.6         | 76.8         | 41.0         | 4.0             | 32.1            | 67.6            | 29.5            | 1.7             | 33.9           | 67.3           | 31.6           | 1.6             |\n|                 | MindAct (Flan-T5 L )         | 53.4         | 75.7         | 50.3         | 7.1             | 39.2            | 67.1            | 35.3            | 1.1             | 39.7           | 67.2           | 37.3           | 2.7             |\n|                 | MindAct (Flan-T5 XL )        | 55.1         | 75.7         | 52.0         | 5.2             | 42.0            | 65.2            | 38.9            | 5.1             | 42.1           | 66.5           | 39.6           | 2.9             |\n|                 | AWM-offline (GPT-4)          | 50.6         | 57.3         | 45.1         | 4.8             | 41.4            | 46.2            | 33.7            | 2.3             | 36.4           | 41.6           | 32.6           | 0.7             |\n| Multi- Stage QA | HTML-T5-XL                   | 60.6         | 81.7         | 57.8         | 10.3            | 47.6            | 71.9            | 42.9            | 5.6             | 50.2           | 74.9           | 48.3           | 5.1             |\n| Multi- Stage QA | Zero-Shot                    |              |              |              |                 |                 |                 |                 |                 |                |                |                |                 |\n| Multi- Stage QA | MindAct (GPT-4)              | 41.6         | 60.6         | 36.2         | 2.0             | 35.8            | 51.1            | 30.1            | 2.0             | 21.6           | 52.8           | 18.6           | 1.0             |\n| Multi- Stage QA | AWM-online (GPT-4)           | 50.0         | 56.4         | 43.6         | 4.0             | 42.1            | 45.1            | 33.9            | 1.6             | 40.9           | 46.3           | 35.5           | 1.7             |\n| Multi- Stage QA | ScribeAgent Small (Ours)     | 42.6         | 50.1         | 39.7         | 0               | 44.9            | 50.1            | 41.6            | 0.6             | 44.1           | 51.4           | 41.4           | 0               |\n| Multi- Stage QA | ScribeAgent Large (Ours)     | 53.5         | 52.9         | 51.2         | 0               | 53.4            | 52.8            | 51.3            | 2.3             | 53.3           | 54.7           | 51.2           | 0               |\n| Direct          | Uses M2W Train Set Flan-T5 B | 20.2         | 52.0         | 17.5         | 0               | 13.9            | 44.7            | 11.0            | 0               | 14.2           | 44.7           | 11.9           | 0.4             |\n| Generation      | Synapse (GPT-3.5)            | 34.0         | -            | 30.6         | 2.4             | 29.1            | -               | 24.2            | 0.6             | 29.6           | -              | 26.4           | 1.5             |\n| Generation      | Zero-Shot                    |              |              |              |                 |                 |                 |                 |                 |                |                |                |                 |\n| Generation      | ScribeAgent Small (Ours)     | 28.6         | 50.1         | 26.8         | 0               | 27.6            | 50.1            | 25.6            | 0               | 32.0           | 51.4           | 29.9           | 0               |\n| Generation      | ScribeAgent Large (Ours)     | 38.0         | 52.9         | 35.6         | 0               | 34.1            | 52.7            | 32.5            | 0               | 39.4           | 54.7           | 37.3           | 0               |\n\n## 4.2 MIND2WEB\n\nMind2Web (Deng et al., 2023) is a text-based dataset for assessing the navigation ability of web agents across different tasks, websites, and domains. Each task features a human demonstration of a real-world workflow, such as booking a hotel on Airbnb. At each step, the agent is asked to predict a single action, consisting of an operation and the target element. Performance is measured by element accuracy, which checks if the correct target is selected; action F1 score, which measures operation correctness like text input; step success rate, which evaluates whether both the target element and the operation are correct; and task success rate, indicating all steps are correct.\n\nThe original Mind2Web benchmark reports two sets of baselines: (1) multi-stage, multi-choice question-answering agents (i.e., the MindAct family) first use a pretrained element-ranking model to filter out 50 candidate elements from the full DOM and then use a separate LLM to recursively select an action from five candidates in a multi-choice question-answering (QA) fashion until one action is chosen; (2) a single-stage, generation-based agent (i.e., fine-tuned Flan-T5 B ) directly generates the operation and the target based on the full DOM. The multi-stage baselines generally show higher metrics than direct generation models, as the element selection process effectively filters out noise, simplifying the task.\n\nBeyond these baselines, we also consider recent published work such as AWM (Wang et al., 2024b), Synapse (Zheng et al., 2024), and fine-tuned HTML-T5 (Gur et al., 2023). AutoWebGLM (Lai et al., 2024) reports only step success rate among all four metrics. While the reported numbers are high, it uses a different and possibly more favorable evaluation procedure, so we do not compare against it. For both single-stage and multi-stage settings, we further categorize the baselines into those leveraging the Mind2Web training data for fine-tuning or in-context demonstrations and zeroshot methods. As stated earlier, we do not fine-tune our agents because our goal is to test the agent's out-of-distribution generalization abilities.\n\nWe evaluate ScribeAgent on both multi-stage QA and direct generation. For the multi-stage setting, we first use the pretrained Mind2Web element-ranker to obtain the element ranking. Then, given the output of ScribeAgent, we traverse the sorted list of HTML elements from top to bottom, and stop when the agent's generated HTML element is a subchild of the element. We then replace ScribeAgent's prediction by the element. For direct generation, we simply compare the output of our agent to the ground truth action and target.\n\nWe report results following the evaluation procedure specified in the Mind2Web repository in Table 6. For the multi-stage setting, ScribeAgent-Large achieves the best overall zero-shot performance. Our element accuracy and step success rate metrics are also competitive with the best\n\nfine-tuned baseline, HTML-T5-XL, on cross-website and cross-domain tasks. However, our task success rates are not satisfactory, which is mainly due to the distribution differences between our training data and the Mind2Web data. Upon inspection, we find that the primary failure cases of our models are (1) predicting the subchild element of the ground truth instead of the ground truth; (2) predicting another element with identical function but is different from the ground truth; and (3) our agent tends to decompose type actions into click followed by type actions. In many cases, we actually correctly predict the action description. These situations can be addressed by improving the evaluation procedure, which we discuss later in this section and in Appendix A.4.2.\n\nAs for direct generation, ScribeAgent-Large outperforms all existing baselines. Our step success rates are 2-3 times higher than those achieved by the fine-tuned Flan-T5 and show an improvement of 5-10% over Synapse, which utilizes GPT-3.5. We attribute ScribeAgent's strong performance to the diversity and high quality of the workflows in our dataset. Relatedly, the three test sets (Cross-Task, Cross-Website, Cross-Domain) are designed to capture different degrees of domain generalization difficulty. Since we do not train on Mind2Web data, the performance of ScribeAgent is similar across all three test sets.\n\nAs mentioned earlier, we observe several limitations in the Mind2Web evaluation that underestimate our agent's performance. First, the evaluation strictly compares element IDs, where only the outermost tags are labeled as ground truths, disregarding the subchildren. This leads to inaccuracies when our agent selects functionally identical but hierarchically deeper elements, which are marked as incorrect. Second, for direct generation, ScribeAgent might select a functionally identical element that is located in a different part of the website (e.g., consider clicking on the next page button vs. clicking on the page number). Lastly, there is a notable discrepancy between the distribution of our training data and the synthetic trajectories of Mind2Web. For instance, Mind2Web expects immediate type actions for textarea or input tags, whereas our model first clicks before typing.\n\nTo better reflect ScribeAgent's capabilities, we refine the evaluation method by relaxing the labels to include subchildren of the ground truths. For direct generation, we also introduce an element attribute matching step that compares not only the element IDs but also tag and text attributes. The results with refined evaluation is shown in Appendix A.4.2. We observe an average of 8% increase in task success rate and element accuracy for ScribeAgent, showing the need for enhancing evaluation of text-based benchmarks. It is worth noting again that results in Table 6 follow the original evaluation procedures to ensure fair comparison with established baselines.\n\nWhile the Mind2Web results are promising, we note that another limitation of static, text-based benchmark is that the ground truth evaluation does not account for different action sequences that could reach the same goal. For instance, to book a flight, one can first enter the destination or first choose the departure date, but the ground truth trajectory only accounts for one possibility. Considering this, we also evaluated ScribeAgent on a dynamic benchmark WebArena (Zhou et al., 2024).\n\n## 4.3 END-TO-END TASK EXECUTION ON WEBARENA\n\nWebArena (Zhou et al., 2024) features 812 web navigation tasks across five domains: E-commerce (OneStopShop), social forums (Reddit), software development (GitLab), content management (CMS), and online map (OpenStreetMap). Unlike the static Mind2Web, it implements a dynamic environment for agents to interact with and allows for assessing the functional accuracy of action sequences. Since the WebArena environment is implemented to accept only target element IDs specified in the accessibility tree, whereas ScribeAgent operates on DOM and outputs targets in HTML, we employ GPT-4o to map between the different representations.\n\nMore generally, we tackle end-to-end task solving by developing a multi-agent system that utilizes GPT-4o to simulate user interactions with ScribeAgent. Our system contains four stages: (1) objective refinement: user adds details about the task objective to help complete the task; (2) action generation: based on the current website and action history, agent outputs an action suggestion; (3) action execution: user executes the suggested action, e.g., clicking a button; (4) completeness evaluation: user observes the current state and decides whether the task is completed.\n\nWe apply the above pipeline to solve the WebArena tasks. In stage 3, GPT-4o maps the agent's output in HTML to the accessibility tree format, which is then processed by the WebArena environ-\n\nTable 7: Task success rates (SR) on WebArena and score breakdown on five web domains. ScribeAgent consistently outperforms considered text-only baselines, often improving the previousbest results by more than 5%. Note that we only test ScribeAgent-Small due to the large number of tasks in WebArena and the evaluation costs associated with the multi-agent system.Table 8: We replace ScribeAgent with GPT-4o in our four-stage pipeline to study how much ScribeAgent contributes to the performance. The success rates drop significantly for all domains.\n\n| Method                      | LLM                        |   Total SR | Shopping   |      | CMS Reddit   | GitLab   | Maps   |\n|-----------------------------|----------------------------|------------|------------|------|--------------|----------|--------|\n| AutoWebGLM                  | ChatGLM3 6B                |       18.2 | -          | -    | -            | -        | -      |\n| NNetnav                     | Llama 3 8B Instruct        |        7.2 | 7.4        | 4.2  | 0            | 0        | 28.5   |\n| AutoEval                    | GPT-4                      |       20.2 | 25.5       | 18.1 | 25.4         | 28.6     | 31.9   |\n| BrowserGym                  | GPT-4                      |       23.5 | -          | -    | -            | -        | -      |\n| BrowserGym axtree           | GPT-4                      |       15   | 17.2       | 14.8 | 20.2         | 19.0     | 25.5   |\n| SteP                        | GPT-4                      |       33   | 37.0       | 24.0 | 59.0         | 32.0     | 30.0   |\n| AWM                         | GPT-4                      |       35.5 | 30.8       | 29.1 | 50.9         | 31.8     | 43.3   |\n| Tree Search                 | GPT-4o                     |       19.2 | -          | -    | -            | -        | -      |\n| WebPilot                    | GPT-4o                     |       37.2 | 36.9       | 24.7 | 65.1         | 39.4     | 33.9   |\n| Broswing+API Hybrid Agent   | GPT-4o                     |       35.8 | 25.7       | 41.2 | 28.3         | 44.4     | 45.9   |\n| AgentOccam with Judge       | GPT-4-Turbo                |       45.7 | 43.3       | 46.2 | 67.0         | 38.9     | 52.3   |\n| Multi-Agent System ( Ours ) | ScribeAgent-Small + GPT-4o |       51.3 | 48.1       | 35.5 | 70.2         | 58.8     | 51.9   |\n| Multi-Agent System ( Ours ) | ScribeAgent-Large + GPT-4o |       53   | 45.8       | 37.9 | 73.7         | 59.7     | 56.3   |\n\n| Method       | LLM                        |   Total SR |   Shopping |   CMS |   Reddit |   GitLab |   Maps |\n|--------------|----------------------------|------------|------------|-------|----------|----------|--------|\n| Single-Agent | GPT-4o                     |       34.2 |       31.9 |  21.3 |     44.7 |     38.2 |   42.6 |\n| Multi-Agent  | ScribeAgent-Small + GPT-4o |       51.3 |       48.1 |  35.5 |     70.2 |     58.8 |   51.9 |\n\nment. To further improve performance, we allow ScribeAgent to generate multiple actions in stage 2 and select the one with the highest confidence using majority vote and GPT-4o analysis. More details about evaluating ScribeAgent on WebArena can be found in Appendix A.5.\n\nWe compare our performance with all top-performing, text-only agents on the WebArena leaderboard. We note that we do not include Autonomous Web Agent (AWA) 1.5 (JaceAI, 2024) as a baseline because it uses a proprietary system to parse the HTML-DOM and web screenshots, rather than building from the WebArena GitHub. This allows them to have richer observations and bypass the accessibility tree action mapping step. In contrast, ScribeAgent is single-modal, text-only, and we stick to the original WebArena implementation. That said, AWA 1.5 employs more advanced reasoning, planning, and progress tracking techniques and is the only agent system with a higher average task success rate than ours.\n\nThe results are shown in Table 7. Compared with existing text-only baselines, ScribeAgent augmented with GPT-4o obtains the highest task success rate in 4 of 5 categories, leading to 7.3% performance improvements in total success rate over the previous-best GPT-4-Turbo-based AgentOccam (Yang et al., 2024b). In particular, on Reddit and GitLab tasks where the domains are more realistic and thus closer to the ones in our training data, ScribeAgent demonstrates stronger generalization ability and higher task success rates than in other domains. Despite known issues with combobox selection and the absence of scroll actions in our training data, our agent effectively navigates these challenges through strategic keyboard actions. More details are provided in Appendix A.5.2.\n\nTo better understand the contribution of ScribeAgent to the multi-agent system, we perform an ablation study that leverages GPT-4o for all four stages of the proposed pipeline. Given the large number of tasks in WebArena and the evaluation costs associated with using a multi-agent system, we perform our ablation studies using ScribeAgent-Small. As shown in Table 8, using ScribeAgent consistently outperforms only using GPT-4o, and the GPT-4o-only setting is less effective than existing agents like WebPilot. This shows that our strong performance on WebArena can be mainly attributed to the action generation process of ScribeAgent. Apart from getting better results, the multi-agent system is cheaper than using GPT-4o alone, as calling ScribeAgent to generate a next action incurs negligible cost as it is served locally.\n\nTable 9: Task success rates on a subset of WebArena. The numbers after the domains indicate the number of tasks considered. All models are used along with GPT-4o to formulate the multi-agent system. We see that the general trends agree with what we found on our proprietary dataset.\n\n| Agent Backbone # Train Tokens Total SR (158) Shopping (36) CMS (39) Reddit (24) GitLab (33) Maps (26)   |    |      |      |      |      |      |      |\n|---------------------------------------------------------------------------------------------------------|----|------|------|------|------|------|------|\n| Mistral 7B                                                                                              | 1B | 41.8 | 41.7 | 30.8 | 50   | 42.4 | 42.3 |\n| Qwen2 7B                                                                                                | 1B | 44.3 | 52.8 | 33.3 | 50   | 48.5 | 42.3 |\n| Qwen2 7B                                                                                                | 3B | 47.5 | 55.6 | 33.3 | 58.3 | 48.5 | 46.2 |\n| Qwen2 7B                                                                                                | 6B | 55   | 58.3 | 41   | 70.8 | 63.6 | 46.2 |\n\nWe also use WebArena to verify the signals observed in our proprietary test data. To do so, we randomly select a subset of 158 WebArena tasks with non-overlapping objective templates and run ablation studies following the ones presented in Section 3.2.4 to study the effect of LLM backbones and the number of training tokens. As shown in Table 9, on all domains, Qwen2 7B outperforms Mistral 7B, and the task success rate increases as the number of training tokens increases. These trends suggest that improvements on our proprietary dataset lead to even greater improvements on WebArena, further highlighting the advantages of fine-tuning web agents with large-scale datasets.\n\n## 5 CONCLUSION\n\nIn this work, we explore how fine-tuning open-source LLMs with high-quality real-world workflow data can benefit developing specialized web agents. We present ScribeAgent, which consistently outperforms existing methods that prompt proprietary models in various evaluation settings and benchmarks. We also provide empirical insights into data processing and model fine-tuning.\n\nLimitations and Future Work. The long-context nature of DOMs presents great challenges in adapting LLMs. In the short term, we aim to enable ScribeAgent to compare and reason over multiple DOM chunks so that its observation is always complete. This might require integrating a memory component, which could also aid in maintaining context or state across interactions to improve multi-step reasoning. Besides, we currently do not incorporate planning into ScribeAgent, so its output will be directly used as the next action. However, adding better action selection strategies such as Monte Carlo Tree Search (MCTS) could potentially facilitate online planning and exploration, further improving the agent's decision-making processes in complex scenarios. In the long run, we aim to expand ScribeAgent's capabilities to handle multi-modal inputs and multilingual content. This would significantly broaden its applicability across different linguistic and visual contexts, making it more versatile and robust in real-world web environments.\n\n## ACKNOWLEDGMENT\n\nWe would like to thank the team at Scribe for their invaluable help and guidance throughout the project. Special thanks go to Thomas Kao for his assistance during the data collection and curation process. We would also like to thank Wayne Chi for helpful discussions that improved the work. This work was supported in part by the National Science Foundation grants IIS1705121, IIS1838017, IIS2046613, IIS2112471, and funding from Meta, Morgan Stanley, Amazon, Google, and Scribe. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of any of these funding agencies.\n\n## REFERENCES\n\nKanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. SeeClick: Harnessing GUI grounding for advanced visual GUI agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 9313-9332, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.505 .\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022. URL https://arxiv.org/abs/2210.11416 .\n\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2023. URL https: //openreview.net/forum?id=kiYqbO3wqw .\n\nAlexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H Laradji, Manuel Del Verme, Tom Marty, L'eo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, et al. Workarena: How capable are web agents at solving common knowledge work tasks? arXiv preprint arXiv:2403.07718 , 2024.\n\nAbhimanyu Dubey, ..., and Zhiwei Zhao. The llama 3 herd of models, 2024. URL https:// arxiv.org/abs/2407.21783 .\n\nYao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, and Honglak Lee. Autoguide: Automated generation and selection of state-aware guidelines for large language model agents. arXiv preprint arXiv:2403.08978 , 2024.\n\nHiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models, 2024. URL https://arxiv.org/abs/2305.11854 .\n\nTeam GLM, ..., and Zihan Wang. Chatglm: A family of large language models from glm-130b to glm-4 all tools, 2024.\n\nIzzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. Understanding html with large language models. ArXiv , abs/2210.03945, 2022. URL https://api.semanticscholar. org/CorpusID:252780086 .\n\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis. ArXiv , abs/2307.12856, 2023. URL https://api.semanticscholar. org/CorpusID:260126067 .\n\nHongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models, 2024. URL https://arxiv.org/abs/2401.13919 .\n\nWenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents, 2023.\n\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations , 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9 .\n\n| JaceAI. Awa 1.5 achieves                                                                                                                                                                                                                                                                                                | breakthrough we-                                                                                                                                                                           |                                                                                                                                     |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|\n| barena benchmark, 2024.                                                                                                                                                                                                                                                                                                 | URL awa-1-5-achieves-breakthrough-performance-on-webarena-benchmark .                                                                                                                      | performance on https://www.jace.ai/post/                                                                                            |\n| model agents. arXiv preprint arXiv:2407.01476                                                                                                                                                                                                                                                                           |                                                                                                                                                                                            | Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language                                      |\n| Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. based web navigating agent. In Discovery and Data Mining , pp. 5295--5306, 2024. Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, | , 2024. Autowebglm: A large language model- Proceedings of the 30th ACM SIGKDD Conference on Knowledge                                                                                     |                                                                                                                                     |\n| and Jie Tang. Webglm: Towards an efficient web-enhanced question answering system with human preferences, 2023. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri with self-feedback, 2023. MistralAI. Announcing mistral 7b, 2023.                                             | Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Ma- jumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement URL | https://mistral.ai/news/                                                                                                            |\n| MistralAI. Codestral: Hello world, 2024a. codestral/ .                                                                                                                                                                                                                                                                  |                                                                                                                                                                                            | https://mistral.ai/news/                                                                                                            |\n| MistralAI. Mixtral of experts, 2024b. mixtral-of-experts/                                                                                                                                                                                                                                                               | URL                                                                                                                                                                                        |                                                                                                                                     |\n| . abs/2410.02907 .                                                                                                                                                                                                                                                                                                      | URL                                                                                                                                                                                        | https://mistral.ai/news/                                                                                                            |\n|                                                                                                                                                                                                                                                                                                                         | demonstrations for web agents without a demonstrator, 2024a. URL                                                                                                                           | Shikhar Murty, Dzmitry Bahdanau, and Christopher D. Manning. Nnetscape navigator: Complex https://arxiv.org/                        |\n|                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                            | Shikhar Murty, Christopher Manning, Peter Shaw, Mandar Joshi, and Kenton Lee. Bagel: Boot- arXiv preprint arXiv:2403.08140 , 2024b. |\n| strapping agents by guiding exploration with language.                                                                                                                                                                                                                                                                  |                                                                                                                                                                                            |                                                                                                                                     |\n|                                                                                                                                                                                                                                                                                                                         | URL                                                                                                                                                                                        |                                                                                                                                     |\n| https://arxiv. .                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                            |                                                                                                                                     |\n| Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,                                                                                                                           |                                                                                                                                                                                            |                                                                                                                                     |\n| Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:                                                                                                                                                                                                                              |                                                                                                                                                                                            |                                                                                                                                     |\n| Browser-assisted question-answering with human feedback, 2022. org/abs/2112.09332                                                                                                                                                                                                                                       |                                                                                                                                                                                            |                                                                                                                                     |\n\nOpenAI. Gpt-4 technical report, 2024a. URL https://arxiv.org/abs/2303.08774 .\n\nOpenAI. Gpt-4o, 2024b. URL https://openai.com/index/hello-gpt-4o/ .\n\nOpenAI. Introducing openai o1, 2024c. URL https://openai.com/o1/ .\n\nTianyue Ou, Frank F. Xu, Aman Madaan, Jiarui Liu, Robert Lo, Abishek Sridhar, Sudipta Sengupta, Dan Roth, Graham Neubig, and Shuyan Zhou. Synatra: Turning indirect knowledge into direct demonstrations for digital agents at scale, 2024. URL https://arxiv.org/abs/2409. 15637 .\n\nJiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474 , 2024.\n\nLeonard Richardson. Beautiful soup documentation. April , 2007.\n\nNicholas Roberts, Samuel Guo, Cong Xu, Ameet Talwalkar, David Lander, Lvfang Tao, Linhang Cai, Shuaicheng Niu, Jianyu Heng, Hongyang Qin, Minwen Deng, Johannes Hog, Alexander Pfefferle, Sushil Ammanaghatta Shivakumar, Arjun Krishnakumar, Yubo Wang, Rhea Sanjay Sukthanker, Frank Hutter, Euxhen Hasanaj, Tien-Dung Le, Mikhail Khodak, Yuriy Nevmyvaka,\n\n| Kashif Rasul, Frederic Sala, Anderson Schneider, Junhong Shen, and Evan R. Sparks. Au- toml decathlon: Diverse tasks, modern methods, and efficiency at scale. In Neural Information Processing Systems , 2021. URL https://api.semanticscholar.org/CorpusID:                                                                                                                                                       |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Baptiste Rozi'ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi                                                                                                                                                                                                                                                                                                                 |\n| Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J'er'emy Rapin, Artyom Kozhevnikov, Ivan Ev- timov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D'efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024. URL https://arxiv.org/abs/2308.12950 . |\n| Junhong Shen, Mikhail Khodak, and Ameet Talwalkar. Efficient architecture search for diverse tasks. In Advances in Neural Information Processing Systems (NeurIPS) , 2022.                                                                                                                                                                                                                                          |\n| Junhong Shen, Liam Li, Lucio M. Dery, Corey Staten, Mikhail Khodak, Graham Neubig, and Ameet Talwalkar. Cross-modal fine-tuning: align then refine. In Proceedings of the 40th International Conference on Machine Learning , 2023.                                                                                                                                                                                 |\n| Junhong Shen, Tanya Marwah, and Ameet Talwalkar. Ups: Towards foundation models for pde solving via cross-modal adaptation. arXiv preprint arXiv:2403.07187 , 2024a. Junhong Shen, Neil Tenenholtz, James Brian Hall, David Alvarez-Melis, and Nicolo Fusi. Tag-llm:                                                                                                                                                |\n| Repurposing general-purpose llms for specialized domains, 2024b. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and                                                                                                                                                                                                                                                              |\n| Paloma Sodhi, S. R. K. Branavan, Yoav Artzi, and Ryan McDonald. Step: Stacked llm policies for web actions. In Conference on Language Modeling (COLM) , 2024. URL https://arxiv. org/abs/2310.03720 . Yueqi Song, Frank Xu, Shuyan Zhou, and Graham Neubig. Beyond browsing: Api-based web                                                                                                                          |\n| agents, 2024. URL .                                                                                                                                                                                                                                                                                                                                                                                                 |\n| https://arxiv.org/abs/2410.16464 Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021.                                                                                                                                                                                                                                                    |\n| Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive plan- ning from feedback with language models. In Thirty-seventh Conference on Neural Information Processing Systems , 2023. URL https://openreview.net/forum?id=rnKgbKmelt . Qwen Team. Qwen2.5: A party of foundation models, September 2024. URL                                                                          |\n| https://qwenlm.                                                                                                                                                                                                                                                                                                                                                                                                     |\n| , 2022.                                                                                                                                                                                                                                                                                                                                                                                                             |\n| Renbo Tu, Nicholas Roberts, Mikhail Khodak, Junhong Shen, Frederic Sala, and Ameet Talwalkar. NAS-bench-360: Benchmarking neural architecture search on diverse tasks. In Advances in Neu- ral Information Processing Systems (NeurIPS) Datasets and Benchmarks Track Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,                                                      |\n| and Anima Anandkumar. Voyager: An open-ended embodied agent with large language mod- els. Transactions on Machine Learning Research , 2024a. ISSN 2835-8856. URL https: //openreview.net/forum?id=ehfRiF0R3a .                                                                                                                                                                                                      |\n| Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. arXiv                                                                                                                                                                                                                                                                                                                             |\n| An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jin-                                                                                                                                              |\n\nRunji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024a. URL https://arxiv.org/abs/2407.10671 .\n\n- Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, and Huzefa Rangwala. Agentoccam: A simple yet strong baseline for llm-based web agents, 2024b. URL https://arxiv.org/abs/2410.13825 .\n- Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. ArXiv , abs/2210.03629, 2022. URL https://api.semanticscholar.org/CorpusID:252762395 .\n\nYao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. Webpilot: A versatile and autonomous multi-agent system for web task execution with strategic exploration, 2024. URL https://arxiv.org/abs/2408.15978 .\n\nBingchen Zhao, Haoqin Tu, Chen Wei, Jieru Mei, and Cihang Xie. Tuning layernorm in attention: Towards efficient multi-modal llm finetuning, 2023. URL https://arxiv.org/abs/ 2312.11420 .\n\nLongtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting with memory for computer control. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id=Pc8AU1aF5e .\n\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id= oKn9c6ytLx .\n\n## A APPENDIX\n\n## A.1 PREPROCESSING\n\n## A.1.1 PRUNING PIPELINE\n\nThe code for preprocessing, chunking the DOM for fine-tuning, fine-tuning, and inference can be found in our GitHub.\n\n## A.1.2 TOKENIZER PRUNING\n\nIn this section, we provide more details on the tokenizer-based detection method to remove random character strings. The rationale behind our approach is based on the observation that typical English words consist of more than two characters. Assuming the token count is t and the character count is s , this means that when t = 1 , s ≥ 2 , leading to s t ≥ 2 . By setting the pruning threshold to 2 and removing tag attributes with s t < 2 , we aim to eliminate strings composed solely of single-character tokens, which are likely to be nonsensical.\n\nIn our actual implementation, we employ this technique only for tag attributes with s > 32 , being more lenient for shorter attributes. To show that this tokenizer pruning strategy is effective and to study the performance across different tokenizers and pruning thresholds, we perform the following experiments.\n\nWe take three tokenizers from different models: Qwen2-7B-Instruct, Mistral-7B-Instruct-v0.3, and Meta-Llama-3-8B. For each tokenizer, we vary the pruning thresholds across a set of values: { 1 . 5 , 1 . 75 , 2 , 2 . 25 , 2 . 5 } . Note that it is meaningless to study overly small thresholds (e.g., it is impossible to have s t < 1 ) or overly large thresholds (e.g., s t < 3 could result in the loss of meaningful attributes, as many English words contain three letters). We randomly sample 1000 DOMs from our proprietary test dataset, apply our standard pruning pipeline followed by tokenizer pruning, and then perform three analysis:\n\n- · False positives: we use the Python enchant library to detect if there are meanful English words within the pruned strings. Note that even though these are actual words, many of them are related to DOM structure and can be safely ignored. Still, we count them as false positives since the tokenizer method is designed to remove random character strings.\n- · Average s and t for the entire DOM before and after tokenizer pruning: this is for understanding the reduction in content length.\n- · Lastly, we sort tags and attributes by the frequency of being pruned to identify patterns.\n\nTable 10: Tokenizer pruning analysis.\n\n| Tokenizer                | Prune Threshold   | False Positive (%) ↓   | Before Pruning (K) s   | Before Pruning (K) s   | After Pruning (K)   | After Pruning (K)   | After Pruning (K)   |\n|--------------------------|-------------------|------------------------|------------------------|------------------------|---------------------|---------------------|---------------------|\n|                          |                   |                        | s                      | t                      |                     | t                   | ∆ t                 |\n| Qwen2-7B-Instruct        | 1.5               | 0.025                  |                        |                        | 221.4               | 77.11               | 2.03                |\n|                          | 1.75              | 0.013                  |                        |                        | 217.3               | 74.67               | 4.47                |\n|                          | 2                 | 0.18                   | 224.3                  | 79.14                  | 215.7               | 73.89               | 5.21                |\n|                          | 2.25              | 0.36                   |                        |                        | 213.9               | 73.13               | 6.01                |\n|                          | 2.5               | 0.38                   |                        |                        | 210.0               | 71.63               | 7.51                |\n| Mistral-7B-Instruct-v0.3 | 1.5               | 0.012                  |                        |                        | 219.5               | 87.10               | 3.44                |\n| Mistral-7B-Instruct-v0.3 | 1.75              | 0.18                   |                        |                        | 216.1               | 85.07               | 5.47                |\n| Mistral-7B-Instruct-v0.3 | 2                 | 0.44                   | 224.3                  | 90.54                  | 212.7               | 83.40               | 7.14                |\n| Mistral-7B-Instruct-v0.3 | 2.25              | 0.49                   |                        |                        | 205.3               | 80.20               | 10.34               |\n| Mistral-7B-Instruct-v0.3 | 2.5               | 11.28                  |                        |                        | 190.3               | 74.44               | 16.10               |\n| Meta-Llama-3-8B          | 1.5               | 0.0097                 |                        |                        | 223.1               | 70.60               | 0.84                |\n| Meta-Llama-3-8B          | 1.75              | 0.012                  |                        |                        | 218.3               | 67.85               | 3.59                |\n| Meta-Llama-3-8B          | 2                 | 0.035                  | 224.3                  | 71.44                  | 216.8               | 67.09               | 3.43                |\n| Meta-Llama-3-8B          | 2.25              | 0.043                  |                        |                        | 215.2               | 66.41               | 5.03                |\n| Meta-Llama-3-8B          | 2.5               | 0.10                   |                        |                        | 212.7               | 65.46               | 5.98                |\n\nAs shown in Table 10, there is a clear trade-off between precision and context reduction: greater reductions in content length tend to result in higher false positive rates. While different tokenizers\n\nexhibit varying sensitivities to the pruning thresholds, a threshold of 2 achieves the most balanced trade-off, which aligns with our intuition. We then list the top-5 tag-attribute pairs most frequently pruned under threshold 2 along with their pruning counts:\n\n- · Qwen: ('div', 'class'): 3188, ('span', 'class'): 11426, ('a', 'href'): 8802, ('button', 'class'): 6844, ('i', 'class'): 5010\n- · Mistral: ('div', 'class'): 5288, ('span', 'class'): 15824, ('a', 'href'): 12948, ('button', 'class'): 7998, ('svg', 'class'): 5871\n- · Llama: ('div', 'class'): 29559, ('span', 'class'): 8823,('button', 'class'): 5889, ('i', 'class'): 4608, ('svg', 'class'): 2577\n\nAttributes such as 'class' often contain random character strings and are frequently pruned. However, we observe differences in how tokenizers handle the href attribute: both Qwen and Mistral tokenizers tend to prune it away, whereas the Llama tokenizer preserves it, indicating its better capability in tokenizing URLs. Although we currently use the Qwen tokenizer in our preprocessing pipeline to align with the backbone model of ScribeAgent, the Llama tokenizer can be a compelling alternative for future consideration since it is better at recognizing URLs and producing shorter token sequences. In general, we believe developing specialized models can be important to achieve strong results, as evidenced in prior works (Shen et al., 2024a; Tu et al., 2022; Shen et al., 2022; Roberts et al., 2021).\n\n## A.2 EXAMPLE PROMPT AND LABEL FOR SCRIBEAGENT\n\n```\nObjective: Grant delegation access to another user in Gmail settings. URL: https://mail.google.com/mail/u/0/ Observation: {processed dom} Step-by-step guide: 1. Description: Click \"See all settings\" Action: mouse\\_click\\_action Node: 254 Target: <button class=\"Tj\" node=\"254\"> 2. Description: Click \"Accounts\" Action: mouse\\_click\\_action Node: 2625 Target: <a class=\"f0 LJOhwe\" href=\"https://mail.google.com/mail/u/0/? tab=#settings/accounts\" node=\"2625\" role=\"tab\"> 3. Description: Click \"Add another account\" Action: mouse\\_click\\_action Node: 1215 Target: <span class=\"LJOhwe sA\" id=\":kp\" node=\"1215\" role=\"link\">\n```\n\n## A.3 OPENAI PROMPTS\n\n## A.3.1 DATA PREPARATION\n\nBelow shows the prompt to generate step descriptions.\n\nYou are navigating a webpage to achieve an objective. Given the objective, a list of the previous actions, the current action, and a screenshot of the current action on the webpage. The objective and previous steps are only here to ground the current step, the current action and its screenshot are the most useful to your task. Give me a concise description of the current action being done on the webpage. You should look at the part of the webpage with the red circle, this is where the user clicked for the current action. Describe this action\n\nand ensure your response is in the same format, concise, coherent. Use any relevant information in the image to ground the action description. Your response should NOT use any json or markdown formatting. The response should be a single sentence that starts with an action verb. For example, 'Click on the 'SUBMIT' button.'\n\nRegenerated Action Descriptions. We provide a few examples of generated action descriptions using GPT-4o.\n\n- · 'Click on the Submit button.'\n- · 'Type in the name of the item.'\n- · 'Double-click on the highlighted text.'\n\n## A.3.2 PROPRIETARY BENCHMARK BASELINES\n\nBelow shows the prompt for all OpenAI baselines. The text is the prepend for every input to which we append the task input with the corresponding objective, URL, DOM, and action history.\n\nYou are an autonomous intelligent agent tasked with solving web-based tasks. These tasks will be accomplished through the use of specific actions you can issue.\n\nHere's the information you'll have:\n\n- - The user's objective: This is the task you're trying to complete.\n- - The current web page's URL: This is the page you're currently navigating.\n- - Part of the current web page's HTML: Each element is assigned in descending order with an unique ID, denoted by the attribute node\\\".\n\nThe actions you can perform include:\n\n- - mouse\\_click\\_action: click\n- - keyboard\\_sequence\\_action: type a sequence of characters\n- - keyboard\\_combination\\_action: press a set of keys together\n\n(e.g., hotkey like ctrl+c) You will generate a step-by-step guide to complete the task based on the given information. You will only produce a SINGLE next step. Do NOT use additional punctuation, or any markdown formatting. The output should be in the following format: Description: Click Users\\\" Action: mouse\\_click\\_action Node: 93\n\nTarget: <a node=\\\"93\\\" class=slds-tree\\_\\_item-label\\\"> Now complete the following task by generating the next step. {task input}\n\n## A.4 MIND2WEB EXPERIMENT DETAILS\n\n## A.4.1 PREPROCESSING\n\nData and Label Conversion. To apply ScribeAgent to Mind2Web data, we first re-process the provided DOM using the procedure detailed in Section 3.2.2. We store a map between our node ID and the backend ID given in the dataset. Then, we transform the history action provided in the dataset to our 5-line format. After ScribeAgent generates the next step, we check the backend ID of the provided label and map it to the node ID in our processed DOM. We then compare this label with the target node ID generated by ScribeAgent. We provide the code for the DOM processing and label conversion process in the supplementary material and will release them later.\n\nDOMChunkingandAction Generation. Whenthe DOM length exceeds the 32K context window, we chunk the DOM sequentially and run the prediction workflow on each piece. For each piece of DOM, we call ScribeAgent five times to obtain five valid actions. We then aggregate all possible actions and select the one with the highest number of appearances. We use the following generation configuration: do sample=True, top p=0.95, temperature=0.6.\n\nTable 11: We also refine the evaluation procedure to better reflect ScribeAgent's capacity.\n\n| Models                   | Eval                                |                                     | Cross-Task   |   Cross-Task | Cross-Task      | Cross-Task   | Cross-Website   | Cross-Website   | Cross-Website   | Cross-Website   | Cross-Domain   | Cross-Domain   | Cross-Domain   | Cross-Domain    |\n|--------------------------|-------------------------------------|-------------------------------------|--------------|--------------|-----------------|--------------|-----------------|-----------------|-----------------|-----------------|----------------|----------------|----------------|-----------------|\n|                          |                                     |                                     | EA AF        |          1   | Step SR Task SR |              |                 | EA AF 1         |                 | Step SR Task SR |                | EA AF 1        |                | Step SR Task SR |\n| Multi- ScribeAgent-Small | M2W                                 | 42.6                                | 50.1         |         39.7 | 0               | 44.9         | 50.1            | 41.6            | 0.6             | 44.1            |                | 51.4           | 41.4           | 0               |\n| Multi- ScribeAgent-Small | M2W+Subchild                        | 42.6                                | 50.1         |         39.8 | 0               | 45.2         | 50.1            | 41.5            | 0.6             | 44.3            |                | 51.4           | 41.6           | 0               |\n| Stage                    | M2W                                 | 53.5                                | 52.9         |         51.2 | 0               | 53.4         | 52.8            | 51.3            | 2.3             |                 | 53.3           | 54.7           | 51.2           | 0               |\n| Stage                    | M2W+Subchild                        | 53.8                                | 52.9         |         51.3 | 0               | 54.0         | 52.8            | 51.9            | 2.3             | 53.5            |                | 54.7           | 51.4           | 0               |\n| Direct                   | M2W                                 | 28.6                                | 50.1         |         26.8 | 0               | 27.6         | 50.1            | 25.6            | 0               | 32.0            |                | 51.4           | 29.9           | 0               |\n| Direct                   | M2W+Subchild + Attr Match 48.8 60.8 |                                     |              |         48.3 | 5.5             | 58.0         | 66.2            | 56.7            | 6.8             | 52.9            |                | 62.1           | 52.4           | 6.5             |\n| Gen                      | M2W                                 | 38.0                                | 52.9         |         35.6 | 0               | 34.1         | 52.7            | 32.5            | 0               | 39.4            |                | 54.7           | 37.3           | 0               |\n| Direct                   | M2W                                 | M2W+Subchild + Attr Match 58.0 63.8 |              |         52   | 5.7             |              | 67.3 69.4       |                 | 59.8            | 11.8            | 62.0           | 63.7           | 52.9           | 10.8            |\n\n## A.4.2 REFINED EVALUATION\n\nAs mentioned in the main text, we improve the Mind2Web evaluation from two perspectives:\n\n- · Subchild label relaxation: We hypothesize that the distribution gap between our training data for ScribeAgent and the Mind2Web test set could be due to Mind2Web preferring ancestor/parent nodes in the HTML tree, while ScribeAgent's training data prefers lower HTML elements. To this effect, we relax the Mind2Web set of positive candidates to include not only the positive candidates, but also their children (direct children and grandchildren).\n- · Attribute matching: Direct generation setting enables higher degree of freedom in element selection. To address scenarios where the predicted element has the same function as the ground truth but is in a different location, we enhance the direct generation evaluation by introducing an element attribute comparison step. Rather than merely comparing the node ID of the predicted and the ground truth elements, we also evaluate the tag and text attributes (e.g., the text displayed on a button). If these attributes match, we consider the prediction to be correct as it has identical functionality.\n\nLastly, we note that in Mind2Web, whenever there is a textarea or an input tag, the expected behavior is to directly execute the type action. However, our model is trained to first click on the input element and then perform the type action. Thus, for actions predicted on textarea or input tags, we adjust our model to replace click actions with type actions and then compare with the ground truths.\n\nTable 11 presents the improved performance of ScribeAgent after refining the evaluation method, showing significant gains in both settings. We find that the label relaxation strategy helps bridge part of the distribution gap, and our multi-stage pipeline effectively covers most of the gains from this label relaxation strategy by using the Mind2Web ranker. However, inspecting cases that are not covered by label relaxation, we found that there still remains a distribution gap. As a result, there is large room for improving the evaluation criteria of text-based benchmark to bridge this gap.\n\n## A.5 WEBARENA EXPERIMENT DETAILS\n\n## A.5.1 FOUR-STAGE PIPELINE\n\nFor the most up-to-date prompts, please refer to our GitHub.\n\nStage 1: GPT-4o refines the intent. We use the following prompt:\n\nI have a simple task objective related to {domain}, rewrite it into a single paragraph of detailed step-by-step actions to achieve the task. When revising the objective, follow the rules:\\\\ - Assume you are already on the correct starting website and are logged in.\\\\ - Do not include any newlines, tabs, step numbers in the rewritten objective.\\\\ - Follow the example as much as possible.\\\\ {In-context demonstrations for domain rules}\\\\ Here is an example:\\\\ Simple Task Objective: {in-context demonstration}\\\\ Detailed Task Objective: {in-context demonstrations}\\\\\n\nNow, rewrite the following objective:\n\nStage 2: We process the environment-generated DOM using our preprocessing procedure. When the DOM length exceeds the 32K context window, we chunk the DOM sequentially and run the prediction workflow on each piece. For each piece of DOM, we call ScribeAgent multiple times to obtain multiple valid actions. We use the following generation configuration: do sample=True, top p=0.95, temperature=0.6. We then aggregate all possible actions, pick the top candidates, and prompt GPT-4o to select the best candidate using the following prompt:\n\nYou are an autonomous agent helping users to solve web-based tasks. These tasks will be accomplished through series of actions. The information you'll have includes:\\\\ - The user's objective\\\\ - The current web page's URL\\\\ - The current web page's accessibility tree\\\\ - Previous steps performed by the user, where each step includes a description of the action and the target web element\\\\ - Several proposed next steps, labeled by ''No.\"\\\\ Your goal is to select the best next step that can complete the task and output this candidate's number, follow the following rules:\\\\ - Do not repeat previous steps\\\\ - Reject candidates with incorrect intentions, e.g., searching for an item different from the one specified in the objective\\\\ - Reject candidates with factual errors, e.g., the description and the chosen web target do not match\\\\ - Only output a single number after to represent the selected candidate but not explanation\\\\ Now analyze the following case:\n\n## Stage 3: GPT-4o maps the agent output to accessibility tree format using the following prompt:\n\nYou are an autonomous agent helping users to solve web-based tasks. These tasks will be accomplished through series of actions. The information you'll have includes:\\\\ - The user's objective\\\\ - The current web page's URL\\\\ - A snippit of the current web page's HTML\\\\ - A snippit of the current web page's accessibility tree\\\\ - Previous steps performed by the user\\\\ Your goal is to translate a proposed next step, which consists of an action and a HTML element, into the following format:\\\\ - 'click [accessibility tree id]': This action clicks on an interactive (non-static) element with a specific id. Note this id is the number inside ''[]\" in the accessibility tree, not the HTML attribute ''node\". Brackets are required in the response. For example, a valid response is ''click [1234]\"\\\\ - 'type [accessibility tree id] [content]': Use this to type the content into the field with a specific id in the accessibility tree. For example, a valid response is ''type [1234] [New York]\". The second bracket should include everything that needs to appear in the textbox, but not only the added content. Do not change the letter case\\\\ - 'press [key\\_comb]': Simulates pressing a key combination on the keyboard (e.g., press [PageDown], press [Enter])\\\\ - 'go\\_back': Return this when the current web page does not contain useful information and the user should go back to the previous web page\\\\ When mapping the next step into actions in the above formats, follow the following rules:\\\\ - Take the user's objective into consideration, so the action must help complete the task\\\\ - Do not repeat previous steps\\\\\n\n- Only output a single step in the above format but not explanation\\\\ Note also: {in-context demonstration of rules}\\\\ Now analyze the following case:\n\nThe action is then returned to the environment for execution.\n\nStage 4: GPT-4o evaluates if the task objective is achieved. For operational tasks, if the task is completed, nothing is returned. For information seeking tasks, if the task is completed, GPT-4o retrieves the answer to the question. The prompt looks like the following:\n\nYou are an autonomous agent helping users to solve web-based tasks. These tasks will be accomplished through series of actions. The information you'll have includes:\\\\ - The user's task, including a high-level objective and a more detailed illustration\\\\\n\n- The current web page's URL and accessibility tree\\\\ - Previous steps performed by the user, where each step includes a description of the action and the target web element\\\\ You should follow the rules: {in-context demonstration rules}\\\\ You will decide whether the task specified by the high-level objective is completed (which means the ** last ** step of the detailed instruction is completed and the current webpage completes the task) and respond ''completed\" or ''incomplete\". If the task requires returning a number or a string and the answer can be obtained in the current webpage, reply ''completed, [answer]\" where ''[answer]\" is the number or string. If the task requires finding a webpage and the current webpage satisfies the requirement, reply ''completed, [answer]\" where ''[answer]\" is the current URL. Now analyze the following case. First provide the reasonings. Then summarize the answer with ''Summary:\", followed by ''completed\" or ''incomplete\", followed by the answer to the question if applicable. Do not include newlines after ''Summary:\".\n\n## A.5.2 SCROLLING ACTIONS AND COMBOBOX SELECTION\n\nIn our data collection process, we capture the full DOM from a system perspective, which inherently includes the entire webpage as observed from the backend. This method differs from user-centric data collection, where only the elements within the visible browser viewport are captured. Consequently, there is no concept of scrolling in our training datasets since all elements are already fully accessible in the captured data.\n\nHowever, we recognize the importance of scroll actions in solving WebArena from a user perspective. To address this, before issuing any action to the environment, our multi-agent system includes a viewport check that uses the bounding box position to determine if the target element is within the visible webpage area. If not, the system manually inserts necessary scroll actions to bring the element into view. This ensures accurate interaction with web elements in a typical user scenario.\n\nTo handle combox selection, our agent discovers a workaround that bypasses the need for scrolling through comboboxes. Specifically, after clicking on the combobox, it types the name of the desired item in the combobox, which brings the item to the top of the dropdown menu. Then, the agent can simply click the item or press Enter. This approach avoids the need for scrolling and is especially effective in densely populated lists. It improves the task success rate on a large number of Map, Reddit, and GitLab tasks.\n\n## A.5.3 GPT-4O-ONLY SETTING\n\n## When we use GPT-4o for stage 2, we use the following prompt:\n\nYou are an autonomous intelligent agent tasked with solving web-based tasks. These tasks will be accomplished through the use of specific actions you can issue. Here's the information you'll have:\\\\\n\n- - The user's objective: This is the task you're trying to complete.\\\\\n- - The current web page's URL: This is the page you're currently navigating.\\\\\n- - The current web page's HTML: Each element is assigned with an unique ID,\n\ndenoted by the attribute ''node\".\\\\\n\nThe actions you can perform include:\\\\\n\n- - mouse\\_click\\_action: click\\\\\n- - keyboard\\_sequence\\_action: type a sequence of characters\\\\\n- - keyboard\\_combination\\_action: press a set of keys together\n\n(e.g., hotkey like ctrl+c)\\\\\n\nYou will generate a step-by-step guide to complete the task based on the given information. At each step, you can perform only one action to one web element. The output should be in the correct format: a single step consisting of a text description, an action, as well as the node and HTML of the target web element to perform the action. Be coherent, concise, and accurate in your response. Do NOT use any special characters (e.g., '' * \", ''#\", etc.) in your response. Follow EXACTLY the format of the response below.\\\\\n\nHere is one example:\\\\ Description: click ''Users\"\\\\ Action: mouse\\_click\\_action\\\\ Node: 93\\\\\n\nTarget: <a class=''slds-tree\\_\\_item-label\" node=''93\">\\\\\n\nNow complete the following task by generating a single next step:\n\n## A.5.4 RESULTS\n\nBelow, we provide example action trajectories produced by our agent and explanations to shed light on its decision making process.\n\nDomain: Shopping\n\nTask ID: 466\n\nIntent: Add 2 Hawaiian Bamboo Orchid Roots #zc50 - by Discount Hawaiian Gifts to my wish list. Actions:\n\n- 1. click [1585] where [1585] is combobox 'Search' autocomplete: both hasPopup: listbox required: False expanded: False → Click on the search bar.\n- 2. type [1585] [Hawaiian Bamboo Orchid Roots #zc50 - by Discount Hawaiian Gifts ] where [1585] is combobox 'Search' focused: True autocomplete: both hasPopup: listbox required: False expanded: True → Type the target item name.\n- 3. press [enter] → Do the search.\n- 4. scroll [down] → Scroll down the page to find out the target item.\n- 5. click [4859] where [4859] is link '2 Hawaiian Bamboo Orchid Roots #zc50 - by Discount Hawaiian Gifts' → Open the item page.\n- 6. click [6733] where [6733] is link 'Add to Wish List' → Add item to wish list.\n\n## Domain: CMS\n\nTask ID: 244\n\nIntent: Show me the email address of the customer who is the most unhappy with Olivia zip jacket.\n\n## Actions:\n\n- 1. click [108] where [108] is link 'MARKETING' → Click on the 'MARKETING' button in the side bar.\n- 2. click [419] where [419] is link 'All Reviews → Clicking the side bar shows more option. We can then click on the 'All Reviews' button.\n- 3. click [2195] where [2195] is button 'Reset Filter' → Remove all previous set filters.\n- 4. type [4435] [olivia] where [4435] is textbox '' required: False → In the search box, type 'olivia'.\n- 5. press [enter] → Do the search\n- 6. stop [emma.lopez@gmail.com] → Inspect all listed reviews and return the email of the negative review.\n\nDomain: Reddit\n\nTask ID: 403\n\nIntent: Change my reddit bio to 'Awesome Prompt Artist'\n\nActions:\n\n- 1. click [12] where [12] is button 'MarvelsGrantMan136' hasPopup: menu expanded: False → Click my user name on the top right corner.\n- 2. click [249] where [249] is link 'Profile' → After the drop-down menu shows up, click 'Profile'.\n- 3. click [886] where [886] is link 'Edit biography' → Click the 'Edit biography' button.\n- 4. type [2456] [Awesome Prompt Artist] where [2456] is textbox 'Biography' required: False describedby: user biography biography help → Type the target content.\n- 5. click [2474] where [2474] is button 'Save' → Save the new profile.\n\nDomain: GitLab\n\nTask ID: 293\n\nIntent: Show me the command to clone Super Awesome Robot with SSH.\n\nActions:\n\n- 1. click [1507] where [1507] is textbox 'Search GitLab' required: False → Click on the search bar.\n- 2. type [1516] [Super Awesome Robot] where [1516] is searchbox 'Search GitLab' focused: True describedby: search-input-description → Type the repo name in the search bar.\n- 3. click [2082] where [2082] is link 'Convex Eggtart / Super Awesome Robot' → Click on the correct repo.\n- 4. click [2699] where [2699] is link 'Clone' → Click on the 'Clone' button.\n- 5. stop [git clone ssh://git@metis.lti.cs.cmu:2222/convexegg/super awesome robot.git] → Read the command from the pop-up window.\n\nDomain: Maps\n\nDomain: Maps\n\nTask ID: 7\n\nIntent: Tell me the full address of all international airports that are within a driving distance of 50 km to Carnegie Mellon University.\n\nActions:\n\n- 1. click [35] where [35] is textbox 'Search' focused: True required: False → Click on the search box.\n- 2. type [35] [airport Pittsburgh] where [35] is textbox 'Search' focused: True required: False → Type 'airport Pittsburgh' in the search box.\n- 3. stop [Pittsburgh International Airport, Airport Boulevard, Findlay Township, Allegheny County, 15231, United States.] → Return 'Pittsburgh International Airport, Airport Boulevard, Findlay Township, Allegheny County, 15231, United States.' as the answer.\n\n**References**:\nhttps://arxiv.org/pdf/2411.15004\n\n**Output**:\n\n<blog>blog content here in markdown format </blog>\n \n<tags>[tag1,tag2]</tags>",
          "additional_kwargs": {},
          "response_metadata": {},
          "type": "human",
          "name": null,
          "id": null,
          "example": false
        },
        {
          "content": "```html\n<blog>\n# ScribeAgent: A Novel Approach to Building Specialized Web Agents\n\n**Abstract:** This blog post delves into the research paper \"SCRIBEAGENT: TOWARDS SPECIALIZED WEB AGENTS USING PRODUCTION-SCALE WORKFLOW DATA.\"  The authors present a novel approach to building web agents, focusing on fine-tuning open-source Large Language Models (LLMs) with real-world workflow data.  This method demonstrates significant performance improvements over existing prompting-based agents on standard web navigation benchmarks.\n\n**Introduction:**  The increasing complexity of web-based tasks necessitates more sophisticated agents capable of independent navigation and execution.  Existing web agents often rely on general-purpose LLMs like GPT-4, leading to limitations in understanding specialized web contexts like HTML and hindering long-term planning.  The ScribeAgent project offers an alternative by fine-tuning open-source LLMs using a massive dataset of real-world user interactions.\n\n**Key Contributions:**\n\n* **Production-Scale Data:** The core strength of ScribeAgent lies in its use of a substantial, proprietary dataset encompassing over 250 domains and 6 billion tokens.  This real-world data, collected through the Scribe platform, provides a rich and diverse set of web interactions, crucial for effective agent training.\n* **Specialized Fine-tuning:**  Instead of relying on prompting general-purpose LLMs, ScribeAgent fine-tunes open-source LLMs using the collected workflow data.  This approach allows for the development of specialized agents tailored to specific web contexts, leading to better performance compared to generic models.\n* **Direct Action Generation:**  ScribeAgent is designed as a single-stage agent, directly generating the next action based on the current DOM and action history.  This contrasts with previous multi-stage approaches, which often involve intermediate steps for element selection. This streamlined approach improves efficiency.\n* **State-of-the-Art Performance:** The paper demonstrates that ScribeAgent achieves state-of-the-art performance on the Mind2Web and WebArena benchmarks.  Significant improvements are reported over existing text-based web agents, highlighting the effectiveness of the proposed method.\n\n**Methodology:**\n\nThe paper details a four-stage pipeline for task execution:\n\n1. **Objective Refinement:**  The user's high-level objective is refined into a more detailed, step-by-step action plan.\n2. **Action Generation:**  ScribeAgent generates a suggested action based on the current web page and history.\n3. **Action Execution:**  The suggested action is executed by the user.\n4. **Completeness Evaluation:**  The system evaluates if the task is completed.\n\n**Discussion:**\n\nThe paper highlights several important considerations:\n\n* **Context Window Optimization:**  The authors address the limitations of context windows by chunking the DOM and using a refined evaluation procedure.\n* **Data Preprocessing:** The detailed preprocessing steps for the workflow data, including removing non-essential elements, are crucial for model effectiveness.\n* **LLM Selection:**  The paper explores different open-source LLMs, demonstrating that fine-tuning can significantly enhance their performance for web navigation tasks.\n\n**Conclusion:**\n\nThe ScribeAgent project offers a promising avenue for developing robust and efficient web agents.  The use of production-scale, real-world data and specialized fine-tuning techniques leads to substantial performance gains over existing methods.  The insights provided in the paper are valuable for future research in the field of web agent development.  Further research could explore integrating memory components, planning strategies, and multi-modal inputs to enhance the capabilities of web agents.\n\n</blog>\n```\n\n<tags>\n[\"web agents\", \"LLMs\", \"web navigation\", \"AI\", \"machine learning\", \"technical\", \"research\", \"computer science\"]\n</tags>\n",
          "additional_kwargs": {},
          "response_metadata": {
            "prompt_feedback": {
              "block_reason": 0,
              "safety_ratings": []
            },
            "finish_reason": "STOP",
            "safety_ratings": [
              {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "probability": "NEGLIGIBLE",
                "blocked": false
              },
              {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "probability": "NEGLIGIBLE",
                "blocked": false
              },
              {
                "category": "HARM_CATEGORY_HARASSMENT",
                "probability": "NEGLIGIBLE",
                "blocked": false
              },
              {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "probability": "NEGLIGIBLE",
                "blocked": false
              }
            ]
          },
          "type": "ai",
          "name": null,
          "id": "run-506fbd8e-25c2-4ac9-826d-2a0fe47196e5-0",
          "example": false,
          "tool_calls": [],
          "invalid_tool_calls": [],
          "usage_metadata": {
            "input_tokens": 51877,
            "output_tokens": 794,
            "total_tokens": 52671,
            "input_token_details": {
              "cache_read": 0
            }
          }
        },
        {
          "content": "The workflow has ended. Thank you!",
          "additional_kwargs": {},
          "response_metadata": {},
          "type": "human",
          "name": null,
          "id": null,
          "example": false
        }
      ],
      "approval_to_generate": true,
      "agent_id": "9776d496-f6fd-44e0-999b-a3dc95c231a9"
    }
  }