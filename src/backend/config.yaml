extractors:
  pdf:
    default:
      class_params: {}
      method_params: {}
    fast:
      class_params:
        timeout: 10
        max_pages: 50
      method_params:
        extract_images: false
        extract_tables: false
  arxiv:
    default:
      class_params: {}
      method_params: {}
  github:
    default:
      class_params: {}
      method_params: {}
  reddit:
    default:
      class_params: {}
      method_params: {}

  html:
    default:
      class_params: {}
      method_params: {}
    minimal:
      class_params:
        strip: ["script", "style", "meta", "link"]
        preserve_newlines: false
      method_params:
        heading_style: "SETEXT"

  text:
    default:
      class_params:
        language: en
        preserve_formatting: true
      method_params:
        clean_whitespace: true
        remove_empty_lines: true
  
converters:
  generic:
    default:
      class_params: {}
      method_params: {}
  pdf:
    default:
      class_params: {}
      method_params: {}
  html:
    default:
      class_params: {}
      method_params: {}
llm:
  default:
    class_params:
      model: groq/llama-3.3-70b-versatile  # Free tier: 30 RPM, 14,400 RPD
      temperature: 0.5
      num_retries: 3  # LiteLLM's built-in retry with exponential backoff
      max_parallel_requests: 5  # Groq has better limits
    method_params: {}
  
  gemini:  # Keep as backup
    class_params:
      model: gemini/gemini-2.5-flash
      temperature: 0.5
      num_retries: 3
      max_parallel_requests: 2
    method_params: {}
  
  gemini-lite:
    class_params:
      model: gemini/gemini-2.5-flash-lite
      temperature: 0.5
      num_retries: 3  # LiteLLM's built-in retry with exponential backoff
      max_parallel_requests: 3  # Flash-lite has 15 RPM (higher limit)
    method_params: {}
  
  gemini-thinking:
    class_params:
      model: gemini/gemini-3-flash-preview
      temperature: 0.5
    method_params: {}
  
  ollama:
    class_params:
      model: ollama/deepseek-r1:1.5b
      temperature: 0.5
      api_base: http://192.168.1.58
    method_params: {}

  chat:
    class_params:
      model: gpt-3.5-turbo
      max_tokens: 1000
      temperature: 0.9
      timeout: 60
      stream: true
    method_params: {}
    
  completion:
    class_params:
      model: text-davinci-003
      max_tokens: 2000
      temperature: 0.5
      timeout: 45
      stream: false
    method_params: {}
