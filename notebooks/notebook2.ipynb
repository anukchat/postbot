{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting websockets\n",
      "  Downloading websockets-14.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Downloading websockets-14.1-cp312-cp312-macosx_11_0_arm64.whl (159 kB)\n",
      "Installing collected packages: websockets\n",
      "Successfully installed websockets-14.1\n"
     ]
    }
   ],
   "source": [
    "%pip install websockets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CheckpointAt' from 'langgraph.checkpoint.base' (/opt/homebrew/Caskroom/miniconda/base/envs/postbot/lib/python3.12/site-packages/langgraph/checkpoint/base/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Any, List\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StateGraph, END\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage, AIMessage\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/postbot/lib/python3.12/site-packages/langgraph/graph/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m END, Graph\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MessageGraph\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StateGraph\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/postbot/lib/python3.12/site-packages/langgraph/graph/graph.py:26\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     Node \u001b[38;5;28;01mas\u001b[39;00m RunnableGraphNode,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchannels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mephemeral_value\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EphemeralValue\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseCheckpointSaver\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TAG_HIDDEN\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpregel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Channel, Pregel\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/postbot/lib/python3.12/site-packages/langgraph/checkpoint/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     BaseCheckpointSaver,\n\u001b[1;32m      3\u001b[0m     Checkpoint,\n\u001b[1;32m      4\u001b[0m     CheckpointAt,\n\u001b[1;32m      5\u001b[0m     SerializerProtocol,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MemorySaver\n\u001b[1;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseCheckpointSaver\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSerializerProtocol\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m ]\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CheckpointAt' from 'langgraph.checkpoint.base' (/opt/homebrew/Caskroom/miniconda/base/envs/postbot/lib/python3.12/site-packages/langgraph/checkpoint/base/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, Any, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, WebSocket, WebSocketDisconnect\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Agent State\n",
    "class AgentState(BaseModel):\n",
    "    tweet: str = \"\"\n",
    "    tweet_category: str = \"\"\n",
    "    selected_style: str = \"\"\n",
    "    draft_blog: str = \"\"\n",
    "    status: str = \"pending\"\n",
    "    human_feedback: str = \"\"\n",
    "\n",
    "# LangGraph Agent Implementation\n",
    "class TweetToBlogAgent:\n",
    "    def __init__(self):\n",
    "        self.model = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "        \n",
    "    def classify_tweet(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Classify the tweet category\"\"\"\n",
    "        response = self.model.invoke([\n",
    "            HumanMessage(content=f\"Classify the following tweet into one of these categories: News, Technology, Personal, Entertainment, Sports, Politics\\n\\nTweet: {state.tweet}\")\n",
    "        ])\n",
    "        state.tweet_category = response.content\n",
    "        return state\n",
    "    \n",
    "    def request_style_selection(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Prepare for human style selection\"\"\"\n",
    "        state.status = \"style_selection_required\"\n",
    "        return state\n",
    "    \n",
    "    def generate_blog(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Generate blog draft based on tweet and selected style\"\"\"\n",
    "        style_prompt = f\"Write a blog post in {state.selected_style} style about the following tweet: {state.tweet}\"\n",
    "        response = self.model.invoke([\n",
    "            HumanMessage(content=style_prompt)\n",
    "        ])\n",
    "        state.draft_blog = response.content\n",
    "        state.status = \"draft_generated\"\n",
    "        return state\n",
    "    \n",
    "    def request_human_review(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Prepare draft for human review\"\"\"\n",
    "        state.status = \"review_required\"\n",
    "        return state\n",
    "    \n",
    "    def publish_blog(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Publish blog after human approval\"\"\"\n",
    "        if state.human_feedback.lower() == \"approve\":\n",
    "            # In a real scenario, you'd integrate with a blog publishing platform\n",
    "            print(f\"Blog Published: {state.draft_blog}\")\n",
    "            state.status = \"published\"\n",
    "        else:\n",
    "            state.status = \"rejected\"\n",
    "        return state\n",
    "\n",
    "    def build_graph(self):\n",
    "        workflow = StateGraph(AgentState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"classify_tweet\", self.classify_tweet)\n",
    "        workflow.add_node(\"request_style_selection\", self.request_style_selection)\n",
    "        workflow.add_node(\"generate_blog\", self.generate_blog)\n",
    "        workflow.add_node(\"request_human_review\", self.request_human_review)\n",
    "        workflow.add_node(\"publish_blog\", self.publish_blog)\n",
    "        \n",
    "        # Add edges\n",
    "        workflow.add_edge(\"classify_tweet\", \"request_style_selection\")\n",
    "        workflow.add_edge(\"request_style_selection\", \"generate_blog\")\n",
    "        workflow.add_edge(\"generate_blog\", \"request_human_review\")\n",
    "        workflow.add_edge(\"request_human_review\", \"publish_blog\")\n",
    "        \n",
    "        workflow.set_entry_point(\"classify_tweet\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"publish_blog\",\n",
    "            lambda state: state.status,\n",
    "            {\n",
    "                \"published\": END,\n",
    "                \"rejected\": \"request_human_review\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return workflow.compile()\n",
    "\n",
    "# FastAPI WebSocket Server\n",
    "class WebSocketServer:\n",
    "    def __init__(self):\n",
    "        self.app = FastAPI()\n",
    "        self.agent = TweetToBlogAgent()\n",
    "        self.graph = self.agent.build_graph()\n",
    "        \n",
    "        @self.app.websocket(\"/process\")\n",
    "        async def websocket_endpoint(websocket: WebSocket):\n",
    "            await websocket.accept()\n",
    "            try:\n",
    "                while True:\n",
    "                    # Receive initial tweet\n",
    "                    data = await websocket.receive_json()\n",
    "                    \n",
    "                    # Initialize agent state\n",
    "                    state = AgentState(tweet=data['tweet'])\n",
    "                    \n",
    "                    # Run through graph\n",
    "                    current_state = state\n",
    "                    for node in self.graph.iterate(current_state):\n",
    "                        current_state = node['state']\n",
    "                        \n",
    "                        # Send status updates via WebSocket\n",
    "                        await websocket.send_json({\n",
    "                            \"status\": current_state.status,\n",
    "                            \"tweet_category\": current_state.tweet_category,\n",
    "                            \"draft_blog\": current_state.draft_blog\n",
    "                        })\n",
    "                        \n",
    "                        # Handle human interaction points\n",
    "                        if current_state.status == \"style_selection_required\":\n",
    "                            await websocket.send_json({\n",
    "                                \"status\": \"awaiting_style_selection\",\n",
    "                                \"tweet_category\": current_state.tweet_category\n",
    "                            })\n",
    "                            # Wait for human style selection\n",
    "                            style_data = await websocket.receive_json()\n",
    "                            current_state.selected_style = style_data['selected_style']\n",
    "                        \n",
    "                        if current_state.status == \"review_required\":\n",
    "                            await websocket.send_json({\n",
    "                                \"status\": \"awaiting_review\",\n",
    "                                \"draft_blog\": current_state.draft_blog\n",
    "                            })\n",
    "                            # Wait for human review\n",
    "                            review_data = await websocket.receive_json()\n",
    "                            current_state.human_feedback = review_data['feedback']\n",
    "                    \n",
    "                    # Final publication status\n",
    "                    await websocket.send_json({\n",
    "                        \"status\": current_state.status,\n",
    "                        \"message\": \"Blog process completed\"\n",
    "                    })\n",
    "                    \n",
    "            except WebSocketDisconnect:\n",
    "                print(\"WebSocket connection closed\")\n",
    "\n",
    "    def run(self, host=\"localhost\", port=8000):\n",
    "        uvicorn.run(self.app, host=host, port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Main execution\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Uncomment one of these to run server or client\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# WebSocketServer().run()\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/postbot/lib/python3.12/asyncio/runners.py:190\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug, loop_factory)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    user_info: str\n",
    "\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            result = self.runnable.invoke(state)\n",
    "            # If the LLM happens to return an empty response, we will re-prompt it\n",
    "            # for an actual response.\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "\n",
    "# Haiku is faster and cheaper, but less accurate\n",
    "# llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "llm = ChatOpenAI(\n",
    "            model=\"llama-3.3-70b-versatile\", \n",
    "            temperature=0.5,\n",
    "            api_key=os.environ[\"GROQ_API_KEY\"],\n",
    "            base_url=\"https://api.groq.com/openai/v1/\"\n",
    "        )\n",
    "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=1)\n",
    "# You can update the LLMs, though you may need to update the prompts\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful customer support assistant for Swiss Airlines. \"\n",
    "            \" Use the provided tools to search for flights, company policies, and other information to assist the user's queries. \"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \" If a search comes up empty, expand your search before giving up.\"\n",
    "            \"\\n\\nCurrent user:\\n<User>\\n{user_info}\\n</User>\"\n",
    "            \"\\nCurrent time: {time}.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now)\n",
    "\n",
    "\n",
    "# \"Read\"-only tools (such as retrievers) don't need a user confirmation to use\n",
    "part_3_safe_tools = [\n",
    "    TavilySearchResults(max_results=1),\n",
    "    fetch_user_flight_information,\n",
    "    search_flights,\n",
    "    lookup_policy,\n",
    "    search_car_rentals,\n",
    "    search_hotels,\n",
    "    search_trip_recommendations,\n",
    "]\n",
    "\n",
    "# These tools all change the user's reservations.\n",
    "# The user has the right to control what decisions are made\n",
    "part_3_sensitive_tools = [\n",
    "    update_ticket_to_new_flight,\n",
    "    cancel_ticket,\n",
    "    book_car_rental,\n",
    "    update_car_rental,\n",
    "    cancel_car_rental,\n",
    "    book_hotel,\n",
    "    update_hotel,\n",
    "    cancel_hotel,\n",
    "    book_excursion,\n",
    "    update_excursion,\n",
    "    cancel_excursion,\n",
    "]\n",
    "sensitive_tool_names = {t.name for t in part_3_sensitive_tools}\n",
    "# Our LLM doesn't have to know which nodes it has to route to. In its 'mind', it's just invoking functions.\n",
    "part_3_assistant_runnable = assistant_prompt | llm.bind_tools(\n",
    "    part_3_safe_tools + part_3_sensitive_tools\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "postbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
